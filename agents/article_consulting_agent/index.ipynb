{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c8f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "521df115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = Groq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\")    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f82f2c",
   "metadata": {},
   "source": [
    "### Setting tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69c278bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker, AgentRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6088600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv # arxiv.org (archive of articles)\n",
    "\n",
    "def article_search(title: str):\n",
    "    search = arxiv.Search(\n",
    "        query=title,\n",
    "        max_results=5,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    results = [\n",
    "        f\"Title: {article.title}\\n\"\n",
    "        f\"Category: {article.primary_category}\\n\"\n",
    "        f\"Link: {article.entry_id}\\n\"\n",
    "        for article in search.results()\n",
    "    ]\n",
    "\n",
    "    return f\"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11a108a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_search_tool = FunctionTool.from_defaults(fn=article_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3dda472",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[article_search_tool],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentRunner(agent_worker)\n",
    "\n",
    "response = agent.chat(\n",
    "    \"Me retorne artigos sobre langchain na educa√ß√£o\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93191f9",
   "metadata": {},
   "source": [
    "### With Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69a74d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "\n",
    "tavily_tool = TavilyToolSpec(\n",
    "    api_key=os.environ.get(\"TAVILY_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "143f8eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n"
     ]
    }
   ],
   "source": [
    "tavily_tool_list = tavily_tool.to_tool_list()\n",
    "\n",
    "for tool in tavily_tool_list:\n",
    "    print(tool.metadata.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8246cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='d7a8999c-e23b-41d8-bbd7-e1c50a6fb00a', embedding=None, metadata={'url': 'https://medium.com/fretebras-tech/langchain-construindo-um-chatpdf-com-rag-retrieval-augmented-generation-openai-e-streamlit-61125e2122de'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Por conta desses acontecimentos, fiquei por um tempo afastado dos meus estudos. Mas por meio desse artigo compartilho meu retorno e comprometimento com os estudos, que sempre gosto de compartilhar por aqui, por meio de artigos, um pouco sobre eles.\\n\\nObrigado pela sua aten√ß√£o! E at√© breve üòÑ\\n\\n--\\n\\n--\\n\\n1\\n\\nPublished in Fretebras Tech [...] LangChain √© uma biblioteca open-source que tem como objetivo facilitar a cria√ß√£o de aplica√ß√µes utilizando LLMs, ou seja, modelos de linguagem.\\n\\nDessa forma, o LangChain disponibiliza diversos tipos de componentes e funcionalidades que facilitam desde a importa√ß√£o de diversos tipos de documentos e dados, cria√ß√£o e reutiliza√ß√£o de templates de promtps, simplificar o uso e conex√£o com uma ou v√°rias LLMs (Como ChatGPT, BERT, Llama, etc). [...] Quem nunca precisou responder a uma d√∫vida que estava em um dos muitos PDFs na pasta de Downloads, e teve que abrir cada um deles e procurar por palavras-chave?\\n\\nNeste artigo, vamos explorar a cria√ß√£o de um ChatPDF utilizando LangChain com a t√©cnica de RAG (Retrieval-Augmented Generation), OpenAI e Streamlit. O texto detalha os conceitos do LangChain e a t√©cnica de RAG, aplicando-os de maneira pr√°tica.\\n\\nLangChain', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1a9fd0d0-7f51-4c8f-a4b5-2db19df94aa1', embedding=None, metadata={'url': 'https://www.youtube.com/watch?v=seei7UaVtA4'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"This month's OpenAI meetup, held on August 27, 2024, featured a discussion led by Michael Pehel. Mike explores how AI tools like ChatGPT and LangChain can streamline content creation, particularly for generating tutorials from YouTube videos and GitHub repositories. Michael demonstrated a method that reduces manual content creation efforts by leveraging AI to handle transcription, topic generation, and code extraction, while addressing challenges like hallucinations and overconfident errors. [...] The session emphasized the importance of careful prompt engineering and selecting the right AI models for specific tasks. [...] Turn Videos into Articles with OpenAI and LangChain \\n Riis LLC \\n 7 likes \\n 121 views \\n 29 Aug 2024\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6c59d625-c8e8-41c6-97e7-6e34220b0cc5', embedding=None, metadata={'url': 'https://www.langchain.com/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[Resources Hub](/resources)[Blog](https://blog.langchain.dev/)[Customer Stories](/customers)[LangChain Academy](https://academy.langchain.com/)[Community](/community)[Experts](/experts)[Changelog](https://changelog.langchain.com/)\\n\\nDocs\\n\\nPython\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)[LangSmith](https://docs.smith.langchain.com/)[LangChain](https://python.langchain.com/docs/introduction/)\\n\\nJavaScript [...] [Resources Hub](/resources)[Blog](https://blog.langchain.dev/)[Customer Stories](/customers)[LangChain Academy](https://academy.langchain.com/)[Community](/community)[Experts](/experts)[Changelog](https://changelog.langchain.com/)\\n\\nDocs\\n\\nPython\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)[LangSmith](https://docs.smith.langchain.com/)[LangChain](https://python.langchain.com/docs/introduction/)\\n\\nJavaScript [...] ![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6723aa76cc7a8e249bd43edf_LIGHT%20BACKGROUND%20-%2031.10.2024%20-%20stack%20diagram.webp)![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667d392696fc0bc3e17a6d04_New%20LC%20stack%20-%20light-2.webp)\\n\\n20M+\\n\\nMonthly Downloads\\n\\n100K+\\n\\nApps Powered\\n\\n100K+\\n\\nGitHub Stars\\n\\n4K+\\n\\nContributors\\n\\n## The biggest developer community in GenAI\\n\\nLearn alongside the 1M+ developers who are pushing the industry forward.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_tool.search(\"Me retorne artigos cientificos sobre LangChain\", max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "370c3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_tool_function = FunctionTool.from_defaults(\n",
    "    fn=tavily_tool.search,\n",
    "    name=\"Tavily search\",\n",
    "    description=\"Busca artigos com Tavily sobre um determinado t√≥pico\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c266094",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[tavily_tool_function],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cc4a7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Me retorne artigos sobre LLM e LangChain\n",
      "=== Calling Function ===\n",
      "Calling function: Tavily search with args: {\"query\": \"LLM e LangChain\", \"max_results\": 10}\n",
      "=== Function Output ===\n",
      "[Document(id_='d6efca82-107d-47ac-95a7-511a7f467aee', embedding=None, metadata={'url': 'https://python.langchain.com/v0.1/docs/modules/model_io/llms/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='On this page\\nLLMs\\nLarge Language Models (LLMs) are a core component of LangChain. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs. To be specific, this interface is one that takes as input a string and returns a string.\\nThere are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them.\\nQuick Start\\u200b [...] Quick Start\\nLLMs\\nCustom LLM\\nCaching\\nStreaming\\nTracking token usage\\nOutput parsers\\n\\n\\n\\n\\n\\n\\n\\nRetrieval\\n\\n\\nDocument loaders\\n\\n\\nText splitters\\n\\n\\nEmbedding models\\n\\n\\nVector stores\\n\\n\\nRetrievers\\n\\n\\nIndexing\\n\\n\\nComposition\\n\\n\\nTools\\n\\n\\nAgents\\n\\n\\nChains\\n\\nMore\\n\\n\\n\\nComponents\\n\\n\\nThis is documentation for LangChain v0.1, which is no longer actively maintained.\\nFor the current stable version, see this version (Latest).\\n\\n\\nModel I/O\\nLLMs [...] LLMs | ü¶úÔ∏èüîó LangChain\\nSkip to main content\\nThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.\\nComponentsIntegrationsGuidesAPI Reference\\nMore\\n\\nPeople\\nVersioning\\nContributing\\nTemplates\\nCookbooks\\nTutorials\\nYouTube\\n\\nv0.1\\n\\nLatest\\nv0.2\\nv0.1\\n\\nü¶úÔ∏èüîó\\n\\nLangSmith\\nLangSmith Docs\\nLangServe GitHub\\nTemplates GitHub\\nTemplates Hub\\nLangChain Hub\\nJS/TS Docs\\n\\nüí¨\\nSearch\\n\\n\\nModel I/O\\n\\n\\nPrompts\\n\\n\\nChat models\\n\\n\\nLLMs', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5b4f123e-7c1b-4e6a-81fb-7f858ed075cf', embedding=None, metadata={'url': 'https://js.langchain.com/v0.1/docs/modules/chains/foundational/llm_chain/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chat\\nSearch\\nThis is documentation for LangChain v0.1, which is no longer actively maintained.\\nFor the current stable version, see this version (Latest).\\nOn this page\\nLLM\\nAn LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents. [...] An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\\nGet started\\u200b\\nWe can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM:\\ntip\\nSee this section for general instructions on installing integration packages. [...] import { OpenAI } from \"@langchain/openai\";import { LLMChain } from \"langchain/chains\";import { PromptTemplate } from \"@langchain/core/prompts\";// We can construct an LLMChain from a PromptTemplate and an LLM.const model = new OpenAI({ temperature: 0 });const prompt = PromptTemplate.fromTemplate(  \"What is a good name for a company that makes {product}?\");const chainA = new LLMChain({ llm: model, prompt });// The result is an object with a `text` property.const resA = await chainA.invoke({', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cf8d9599-2ac9-4879-bada-4c1358d5141b', embedding=None, metadata={'url': 'https://scalexi.medium.com/understanding-the-differences-between-llm-chains-and-llm-agent-executors-in-langchain-3f3cf402442f'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"LangChain has emerged as a robust framework for building applications powered by large language models (LLMs). Two fundamental concepts within LangChain are LLM Chains and LLM Agent Executors, both of which leverage tools to enhance the capabilities of LLMs. While they may seem similar at first glance, understanding their differences is crucial for developers aiming to harness LangChain's full potential. [...] Both LLM Chains and LLM Agent Executors offer powerful ways to structure and execute tasks using LangChain, but they are designed for different use cases. Understanding the key differences is essential for choosing the right approach. [...] By selecting the appropriate architecture ‚Äî whether it‚Äôs the simplicity of LLM Chains or the adaptability of LLM Agent Executors ‚Äî you can build more efficient, intelligent, and effective applications using LangChain, tailored to the specific needs of your project.\\n\\nIf you need further information or assistance, feel free to contact ScaleX Innovation at info@scalexi.ai.\\n\\n--\\n\\n--\\n\\n2\\n\\nWritten by ScaleX Innovation\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ed77cbf3-6f24-4672-92fa-a4536f1ae567', embedding=None, metadata={'url': 'https://www.stardog.com/blog/designing-llm-applications-with-knowledge-graphs-and-langchain/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LangChain is described as ‚Äúa framework for developing applications powered by language models‚Äù ‚Äî which is precisely how we use it within Voicebox. Its two central concepts for us are Chain and Vectorstore. [...] Summary\\n\\nLangChain has been a lot of fun to work with in Voicebox. It fit our needs for a framework in Python, allowing us to quickly transition from primitive, low-level code working directly with the LLM and getting every bit of mileage out of f-strings for prompt creation, to something more structured and extensible. [...] Vectorstore is pretty obvious; everyone has one in their stack. The LangChain API is well designed, and it was a trivial task for us to add support for txtai, a vector database with which we‚Äôd already had a lot of experience and tooling. This removed some learning curve hurdles, and we were able to iterate faster to more sophisticated versions of Voicebox‚Äôs core features.\\n\\nA Chain encapsulates the logic of using an LLM to accomplish a specific task. From the LangChain site:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='21aa0bf9-58ea-4542-a82c-97f9d4c251ac', embedding=None, metadata={'url': 'https://www.langchain.com/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Build\\n\\nLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.\\n\\nRun\\n\\nDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.\\n\\nManage\\n\\nLangSmith is a unified agent observability and evals platform to optimize the performance of your\\xa0AI agents - whether they're built with a LangChain framework or not. [...] ## The reference architecture enterprises adopt for success.\\n\\nLangChain‚Äôs suite of products can be used independently or stacked together for multiplicative impact ‚Äì guiding you through building, running, and managing your LLM apps. [...] [](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ce012c99a9683732d528cf_retrieval-video-transcode.mp4)\\n\\n## Build your app with LangChain\\n\\nBuild context-aware, reasoning applications with LangChain‚Äôs flexible framework that leverages your company‚Äôs data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.\\n\\n[Learn more about LangChain](/langchain)\\n\\n## Run at scale with LangGraph\\xa0Platform\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f8a021eb-f623-4c8a-976c-65f18aed2315', embedding=None, metadata={'url': 'https://www.pinecone.io/learn/series/langchain/langchain-prompt-templates/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='question cannot be answered using the information provided answer\\nwith \"I don\\'t know\".\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\nTheir superior performance over smaller models has made them incredibly\\nuseful for developers building NLP enabled applications. These models\\ncan be accessed via Hugging Face\\'s transformers library, via OpenAI\\nusing the openai library, and via Cohere using the cohere library.\\nQuestion: Which libraries and model providers offer LLMs? [...] template = \"\"\"Answer the question based on the context below. If the\\nquestion cannot be answered using the information provided answer\\nwith \"I don\\'t know\".\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\nTheir superior performance over smaller models has made them incredibly\\nuseful for developers building NLP enabled applications. These models\\ncan be accessed via Hugging Face\\'s transformers library, via OpenAI [...] prompt = \"\"\"Answer the question based on the context below. If the\\nquestion cannot be answered using the information provided answer\\nwith \"I don\\'t know\".\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\nTheir superior performance over smaller models has made them incredibly\\nuseful for developers building NLP enabled applications. These models\\ncan be accessed via Hugging Face\\'s transformers library, via OpenAI', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f232dcb0-4a78-4567-aa04-06cec59d9604', embedding=None, metadata={'url': 'https://python.langchain.com/docs/integrations/llms/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"LLMs\\ncaution\\nYou are currently on a page documenting the use of text completion models. Many of the latest and most popular models are chat completion models.\\nUnless you are specifically using more advanced prompting techniques, you are probably looking for this page instead.\\nLLMs are language models that take a string as input and return a string as output.\\ninfo\\nIf you'd like to write your own LLM, see this how-to. If you'd like to contribute an integration, see Contributing integrations. [...] | Yuan2.0 | Yuan2.0 is a new generation Fundamental Large Language Model develope... |\\nEdit this page [...] | Provider | Package |\\n| --- | --- |\\n| AI21LLM | langchain-ai21 |\\n| AnthropicLLM | langchain-anthropic |\\n| AzureOpenAI | langchain-openai |\\n| BedrockLLM | langchain-aws |\\n| CohereLLM | langchain-cohere |\\n| FireworksLLM | langchain-fireworks |\\n| OllamaLLM | langchain-ollama |\\n| OpenAILLM | langchain-openai |\\n| TogetherLLM | langchain-together |\\n| VertexAILLM | langchain-google_vertexai |\\n| NVIDIA | langchain-nvidia |\\nAll LLMs\\u200b\\n| Name | Description |\\n| --- | --- |\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='616cd9a6-0236-4ac1-b856-11965e3bdeda', embedding=None, metadata={'url': 'https://www.youtube.com/watch?v=AjQPRomyd-k'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"LLM Project | End to End Gen AI Project Using LangChain, Google Palm In Ed-Tech Industry \\n codebasics \\n 2910 likes \\n 119522 views \\n 27 Oct 2023 \\n This is an End-to-End LLM project using the langchain framework. We are building a question-and-answer system for a real e-learning company (no toy datasets). If you are new to GenAI and LLM application development, then this langchain tutorial will help you understand how to build the end-to-end application. [...] we are going to build end to endend llm project where we will use Lang chain hugging face streamlet and Google Palm model the use case we are solving is for a real e-learning company we have not used any toy data set here that company is my own company code Basics we have this code basic. platform which has many data related courses with more than 20,000 students now these existing students or a person who wants to buy the course they will have questions related to courses fees Etc and we will [...] this requirement. txt file see this txt file has all these modules which will be using in our project and you can go to this folder and simply run p install r hyph r requirement. txt and it will install all the modules by the way I'm giving all this code check in video description you'll be able to download all this code okay it has this faq.com text file and from this place now I'm going to import Google prom so you'll say from Lang chain. llm import Google pal and then llm object is equal to\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b304a8b2-590f-48d4-8950-dfbbb0e41b20', embedding=None, metadata={'url': 'https://www.reddit.com/r/LangChain/comments/1al25wz/langchain_for_llm/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='You can use pretty much most of the new fully local models. Langchain supports Ollama through their open ai API support so you can use RAG using', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0f98c5e7-43a5-4c5c-9898-a63a5fe8eae6', embedding=None, metadata={'url': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Whether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to the global verbose value,\\naccessible via langchain.globals.get_verbose().\\n\\nCreate LLMChain from LLM and template.\\n\\nllm (BaseLanguageModel)\\n\\ntemplate (str)\\n\\nLLMChain\\n\\nDeprecated since version 0.1.0: Use invoke() instead. It will not be removed until langchain==1.0.\\n\\nExecute the chain. [...] Section Navigation\\n\\nBase packages\\n\\nIntegrations\\n\\nLLMChain#\\n\\nBases: Chain\\n\\nDeprecated since version 0.1.17: Use , `prompt | llm`() instead. It will not be removed until langchain==1.0.\\n\\nChain to run queries against LLMs.\\n\\nThis class is deprecated. See below for an example implementation using\\nLangChain runnables:\\n\\nExample\\n\\nNote\\n\\nLLMChain implements the standard Runnable Interface. üèÉ [...] Call apply and then parse the results.\\n\\ninput_list (List[Dict[str, Any]])\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\nSequence[str | List[str] | Dict[str, str]]\\n\\nCall apredict and then parse the results.\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\nkwargs (Any)\\n\\nstr | List[str] | Dict[str, str]\\n\\nPrepare chain inputs, including adding inputs from memory.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n",
      "=== LLM Response ===\n",
      "Aqui est√£o alguns artigos sobre LLM e LangChain:\n",
      "\n",
      "1. \"LLMs\" da documenta√ß√£o do LangChain: https://python.langchain.com/v0.1/docs/modules/model_io/llms/\n",
      "2. \"LLM Chain\" da documenta√ß√£o do LangChain: https://js.langchain.com/v0.1/docs/modules/chains/foundational/llm_chain/\n",
      "3. \"Designing LLM Applications with Knowledge Graphs and LangChain\" do blog da Stardog: https://www.stardog.com/blog/designing-llm-applications-with-knowledge-graphs-and-langchain/\n",
      "4. \"LangChain\" do site oficial do LangChain: https://www.langchain.com/\n",
      "5. \"LLM Project | End to End Gen AI Project Using LangChain, Google Palm In Ed-Tech Industry\" do YouTube: https://www.youtube.com/watch?v=AjQPRomyd-k\n",
      "6. \"LangChain for LLM\" do Reddit: https://www.reddit.com/r/LangChain/comments/1al25wz/langchain_for_llm/\n",
      "7. \"LLMChain\" da documenta√ß√£o do LangChain: https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html\n",
      "\n",
      "Esses artigos fornecem informa√ß√µes sobre como usar o LangChain para desenvolver aplica√ß√µes com LLMs, incluindo exemplos de c√≥digo e tutoriais. Al√©m disso, eles discutem as vantagens e desvantagens de usar o LangChain e como ele pode ser utilizado em diferentes contextos.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='Aqui est√£o alguns artigos sobre LLM e LangChain:\\n\\n1. \"LLMs\" da documenta√ß√£o do LangChain: https://python.langchain.com/v0.1/docs/modules/model_io/llms/\\n2. \"LLM Chain\" da documenta√ß√£o do LangChain: https://js.langchain.com/v0.1/docs/modules/chains/foundational/llm_chain/\\n3. \"Designing LLM Applications with Knowledge Graphs and LangChain\" do blog da Stardog: https://www.stardog.com/blog/designing-llm-applications-with-knowledge-graphs-and-langchain/\\n4. \"LangChain\" do site oficial do LangChain: https://www.langchain.com/\\n5. \"LLM Project | End to End Gen AI Project Using LangChain, Google Palm In Ed-Tech Industry\" do YouTube: https://www.youtube.com/watch?v=AjQPRomyd-k\\n6. \"LangChain for LLM\" do Reddit: https://www.reddit.com/r/LangChain/comments/1al25wz/langchain_for_llm/\\n7. \"LLMChain\" da documenta√ß√£o do LangChain: https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html\\n\\nEsses artigos fornecem informa√ß√µes sobre como usar o LangChain para desenvolver aplica√ß√µes com LLMs, incluindo exemplos de c√≥digo e tutoriais. Al√©m disso, eles discutem as vantagens e desvantagens de usar o LangChain e como ele pode ser utilizado em diferentes contextos.', sources=[ToolOutput(content='[Document(id_=\\'d6efca82-107d-47ac-95a7-511a7f467aee\\', embedding=None, metadata={\\'url\\': \\'https://python.langchain.com/v0.1/docs/modules/model_io/llms/\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\\'On this page\\\\nLLMs\\\\nLarge Language Models (LLMs) are a core component of LangChain. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs. To be specific, this interface is one that takes as input a string and returns a string.\\\\nThere are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them.\\\\nQuick Start\\\\u200b [...] Quick Start\\\\nLLMs\\\\nCustom LLM\\\\nCaching\\\\nStreaming\\\\nTracking token usage\\\\nOutput parsers\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nRetrieval\\\\n\\\\n\\\\nDocument loaders\\\\n\\\\n\\\\nText splitters\\\\n\\\\n\\\\nEmbedding models\\\\n\\\\n\\\\nVector stores\\\\n\\\\n\\\\nRetrievers\\\\n\\\\n\\\\nIndexing\\\\n\\\\n\\\\nComposition\\\\n\\\\n\\\\nTools\\\\n\\\\n\\\\nAgents\\\\n\\\\n\\\\nChains\\\\n\\\\nMore\\\\n\\\\n\\\\n\\\\nComponents\\\\n\\\\n\\\\nThis is documentation for LangChain v0.1, which is no longer actively maintained.\\\\nFor the current stable version, see this version (Latest).\\\\n\\\\n\\\\nModel I/O\\\\nLLMs [...] LLMs | ü¶úÔ∏èüîó LangChain\\\\nSkip to main content\\\\nThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.\\\\nComponentsIntegrationsGuidesAPI Reference\\\\nMore\\\\n\\\\nPeople\\\\nVersioning\\\\nContributing\\\\nTemplates\\\\nCookbooks\\\\nTutorials\\\\nYouTube\\\\n\\\\nv0.1\\\\n\\\\nLatest\\\\nv0.2\\\\nv0.1\\\\n\\\\nü¶úÔ∏èüîó\\\\n\\\\nLangSmith\\\\nLangSmith Docs\\\\nLangServe GitHub\\\\nTemplates GitHub\\\\nTemplates Hub\\\\nLangChain Hub\\\\nJS/TS Docs\\\\n\\\\nüí¨\\\\nSearch\\\\n\\\\n\\\\nModel I/O\\\\n\\\\n\\\\nPrompts\\\\n\\\\n\\\\nChat models\\\\n\\\\n\\\\nLLMs\\', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'5b4f123e-7c1b-4e6a-81fb-7f858ed075cf\\', embedding=None, metadata={\\'url\\': \\'https://js.langchain.com/v0.1/docs/modules/chains/foundational/llm_chain/\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\\'Chat\\\\nSearch\\\\nThis is documentation for LangChain v0.1, which is no longer actively maintained.\\\\nFor the current stable version, see this version (Latest).\\\\nOn this page\\\\nLLM\\\\nAn LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents. [...] An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\\\\nGet started\\\\u200b\\\\nWe can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM:\\\\ntip\\\\nSee this section for general instructions on installing integration packages. [...] import { OpenAI } from \"@langchain/openai\";import { LLMChain } from \"langchain/chains\";import { PromptTemplate } from \"@langchain/core/prompts\";// We can construct an LLMChain from a PromptTemplate and an LLM.const model = new OpenAI({ temperature: 0 });const prompt = PromptTemplate.fromTemplate(  \"What is a good name for a company that makes {product}?\");const chainA = new LLMChain({ llm: model, prompt });// The result is an object with a `text` property.const resA = await chainA.invoke({\\', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'cf8d9599-2ac9-4879-bada-4c1358d5141b\\', embedding=None, metadata={\\'url\\': \\'https://scalexi.medium.com/understanding-the-differences-between-llm-chains-and-llm-agent-executors-in-langchain-3f3cf402442f\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\"LangChain has emerged as a robust framework for building applications powered by large language models (LLMs). Two fundamental concepts within LangChain are LLM Chains and LLM Agent Executors, both of which leverage tools to enhance the capabilities of LLMs. While they may seem similar at first glance, understanding their differences is crucial for developers aiming to harness LangChain\\'s full potential. [...] Both LLM Chains and LLM Agent Executors offer powerful ways to structure and execute tasks using LangChain, but they are designed for different use cases. Understanding the key differences is essential for choosing the right approach. [...] By selecting the appropriate architecture ‚Äî whether it‚Äôs the simplicity of LLM Chains or the adaptability of LLM Agent Executors ‚Äî you can build more efficient, intelligent, and effective applications using LangChain, tailored to the specific needs of your project.\\\\n\\\\nIf you need further information or assistance, feel free to contact ScaleX Innovation at info@scalexi.ai.\\\\n\\\\n--\\\\n\\\\n--\\\\n\\\\n2\\\\n\\\\nWritten by ScaleX Innovation\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'ed77cbf3-6f24-4672-92fa-a4536f1ae567\\', embedding=None, metadata={\\'url\\': \\'https://www.stardog.com/blog/designing-llm-applications-with-knowledge-graphs-and-langchain/\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\\'LangChain is described as ‚Äúa framework for developing applications powered by language models‚Äù ‚Äî which is precisely how we use it within Voicebox. Its two central concepts for us are Chain and Vectorstore. [...] Summary\\\\n\\\\nLangChain has been a lot of fun to work with in Voicebox. It fit our needs for a framework in Python, allowing us to quickly transition from primitive, low-level code working directly with the LLM and getting every bit of mileage out of f-strings for prompt creation, to something more structured and extensible. [...] Vectorstore is pretty obvious; everyone has one in their stack. The LangChain API is well designed, and it was a trivial task for us to add support for txtai, a vector database with which we‚Äôd already had a lot of experience and tooling. This removed some learning curve hurdles, and we were able to iterate faster to more sophisticated versions of Voicebox‚Äôs core features.\\\\n\\\\nA Chain encapsulates the logic of using an LLM to accomplish a specific task. From the LangChain site:\\', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'21aa0bf9-58ea-4542-a82c-97f9d4c251ac\\', embedding=None, metadata={\\'url\\': \\'https://www.langchain.com/\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\"Build\\\\n\\\\nLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.\\\\n\\\\nRun\\\\n\\\\nDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.\\\\n\\\\nManage\\\\n\\\\nLangSmith is a unified agent observability and evals platform to optimize the performance of your\\\\xa0AI agents - whether they\\'re built with a LangChain framework or not. [...] ## The reference architecture enterprises adopt for success.\\\\n\\\\nLangChain‚Äôs suite of products can be used independently or stacked together for multiplicative impact ‚Äì guiding you through building, running, and managing your LLM apps. [...] [](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ce012c99a9683732d528cf_retrieval-video-transcode.mp4)\\\\n\\\\n## Build your app with LangChain\\\\n\\\\nBuild context-aware, reasoning applications with LangChain‚Äôs flexible framework that leverages your company‚Äôs data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.\\\\n\\\\n[Learn more about LangChain](/langchain)\\\\n\\\\n## Run at scale with LangGraph\\\\xa0Platform\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'f8a021eb-f623-4c8a-976c-65f18aed2315\\', embedding=None, metadata={\\'url\\': \\'https://www.pinecone.io/learn/series/langchain/langchain-prompt-templates/\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\\'question cannot be answered using the information provided answer\\\\nwith \"I don\\\\\\'t know\".\\\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\\\nTheir superior performance over smaller models has made them incredibly\\\\nuseful for developers building NLP enabled applications. These models\\\\ncan be accessed via Hugging Face\\\\\\'s transformers library, via OpenAI\\\\nusing the openai library, and via Cohere using the cohere library.\\\\nQuestion: Which libraries and model providers offer LLMs? [...] template = \"\"\"Answer the question based on the context below. If the\\\\nquestion cannot be answered using the information provided answer\\\\nwith \"I don\\\\\\'t know\".\\\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\\\nTheir superior performance over smaller models has made them incredibly\\\\nuseful for developers building NLP enabled applications. These models\\\\ncan be accessed via Hugging Face\\\\\\'s transformers library, via OpenAI [...] prompt = \"\"\"Answer the question based on the context below. If the\\\\nquestion cannot be answered using the information provided answer\\\\nwith \"I don\\\\\\'t know\".\\\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\\\nTheir superior performance over smaller models has made them incredibly\\\\nuseful for developers building NLP enabled applications. These models\\\\ncan be accessed via Hugging Face\\\\\\'s transformers library, via OpenAI\\', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'f232dcb0-4a78-4567-aa04-06cec59d9604\\', embedding=None, metadata={\\'url\\': \\'https://python.langchain.com/docs/integrations/llms/\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\"LLMs\\\\ncaution\\\\nYou are currently on a page documenting the use of text completion models. Many of the latest and most popular models are chat completion models.\\\\nUnless you are specifically using more advanced prompting techniques, you are probably looking for this page instead.\\\\nLLMs are language models that take a string as input and return a string as output.\\\\ninfo\\\\nIf you\\'d like to write your own LLM, see this how-to. If you\\'d like to contribute an integration, see Contributing integrations. [...] | Yuan2.0 | Yuan2.0 is a new generation Fundamental Large Language Model develope... |\\\\nEdit this page [...] | Provider | Package |\\\\n| --- | --- |\\\\n| AI21LLM | langchain-ai21 |\\\\n| AnthropicLLM | langchain-anthropic |\\\\n| AzureOpenAI | langchain-openai |\\\\n| BedrockLLM | langchain-aws |\\\\n| CohereLLM | langchain-cohere |\\\\n| FireworksLLM | langchain-fireworks |\\\\n| OllamaLLM | langchain-ollama |\\\\n| OpenAILLM | langchain-openai |\\\\n| TogetherLLM | langchain-together |\\\\n| VertexAILLM | langchain-google_vertexai |\\\\n| NVIDIA | langchain-nvidia |\\\\nAll LLMs\\\\u200b\\\\n| Name | Description |\\\\n| --- | --- |\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'616cd9a6-0236-4ac1-b856-11965e3bdeda\\', embedding=None, metadata={\\'url\\': \\'https://www.youtube.com/watch?v=AjQPRomyd-k\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\"LLM Project | End to End Gen AI Project Using LangChain, Google Palm In Ed-Tech Industry \\\\n codebasics \\\\n 2910 likes \\\\n 119522 views \\\\n 27 Oct 2023 \\\\n This is an End-to-End LLM project using the langchain framework. We are building a question-and-answer system for a real e-learning company (no toy datasets). If you are new to GenAI and LLM application development, then this langchain tutorial will help you understand how to build the end-to-end application. [...] we are going to build end to endend llm project where we will use Lang chain hugging face streamlet and Google Palm model the use case we are solving is for a real e-learning company we have not used any toy data set here that company is my own company code Basics we have this code basic. platform which has many data related courses with more than 20,000 students now these existing students or a person who wants to buy the course they will have questions related to courses fees Etc and we will [...] this requirement. txt file see this txt file has all these modules which will be using in our project and you can go to this folder and simply run p install r hyph r requirement. txt and it will install all the modules by the way I\\'m giving all this code check in video description you\\'ll be able to download all this code okay it has this faq.com text file and from this place now I\\'m going to import Google prom so you\\'ll say from Lang chain. llm import Google pal and then llm object is equal to\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'b304a8b2-590f-48d4-8950-dfbbb0e41b20\\', embedding=None, metadata={\\'url\\': \\'https://www.reddit.com/r/LangChain/comments/1al25wz/langchain_for_llm/\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\\'You can use pretty much most of the new fully local models. Langchain supports Ollama through their open ai API support so you can use RAG using\\', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'0f98c5e7-43a5-4c5c-9898-a63a5fe8eae6\\', embedding=None, metadata={\\'url\\': \\'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\\'Whether or not run in verbose mode. In verbose mode, some intermediate logs\\\\nwill be printed to the console. Defaults to the global verbose value,\\\\naccessible via langchain.globals.get_verbose().\\\\n\\\\nCreate LLMChain from LLM and template.\\\\n\\\\nllm (BaseLanguageModel)\\\\n\\\\ntemplate (str)\\\\n\\\\nLLMChain\\\\n\\\\nDeprecated since version 0.1.0: Use invoke() instead. It will not be removed until langchain==1.0.\\\\n\\\\nExecute the chain. [...] Section Navigation\\\\n\\\\nBase packages\\\\n\\\\nIntegrations\\\\n\\\\nLLMChain#\\\\n\\\\nBases: Chain\\\\n\\\\nDeprecated since version 0.1.17: Use , `prompt | llm`() instead. It will not be removed until langchain==1.0.\\\\n\\\\nChain to run queries against LLMs.\\\\n\\\\nThis class is deprecated. See below for an example implementation using\\\\nLangChain runnables:\\\\n\\\\nExample\\\\n\\\\nNote\\\\n\\\\nLLMChain implements the standard Runnable Interface. üèÉ [...] Call apply and then parse the results.\\\\n\\\\ninput_list (List[Dict[str, Any]])\\\\n\\\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\\\n\\\\nSequence[str | List[str] | Dict[str, str]]\\\\n\\\\nCall apredict and then parse the results.\\\\n\\\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\\\n\\\\nkwargs (Any)\\\\n\\\\nstr | List[str] | Dict[str, str]\\\\n\\\\nPrepare chain inputs, including adding inputs from memory.\\', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\')]', tool_name='Tavily search', raw_input={'args': (), 'kwargs': {'query': 'LLM e LangChain', 'max_results': 10}}, raw_output=[Document(id_='d6efca82-107d-47ac-95a7-511a7f467aee', embedding=None, metadata={'url': 'https://python.langchain.com/v0.1/docs/modules/model_io/llms/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='On this page\\nLLMs\\nLarge Language Models (LLMs) are a core component of LangChain. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs. To be specific, this interface is one that takes as input a string and returns a string.\\nThere are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them.\\nQuick Start\\u200b [...] Quick Start\\nLLMs\\nCustom LLM\\nCaching\\nStreaming\\nTracking token usage\\nOutput parsers\\n\\n\\n\\n\\n\\n\\n\\nRetrieval\\n\\n\\nDocument loaders\\n\\n\\nText splitters\\n\\n\\nEmbedding models\\n\\n\\nVector stores\\n\\n\\nRetrievers\\n\\n\\nIndexing\\n\\n\\nComposition\\n\\n\\nTools\\n\\n\\nAgents\\n\\n\\nChains\\n\\nMore\\n\\n\\n\\nComponents\\n\\n\\nThis is documentation for LangChain v0.1, which is no longer actively maintained.\\nFor the current stable version, see this version (Latest).\\n\\n\\nModel I/O\\nLLMs [...] LLMs | ü¶úÔ∏èüîó LangChain\\nSkip to main content\\nThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.\\nComponentsIntegrationsGuidesAPI Reference\\nMore\\n\\nPeople\\nVersioning\\nContributing\\nTemplates\\nCookbooks\\nTutorials\\nYouTube\\n\\nv0.1\\n\\nLatest\\nv0.2\\nv0.1\\n\\nü¶úÔ∏èüîó\\n\\nLangSmith\\nLangSmith Docs\\nLangServe GitHub\\nTemplates GitHub\\nTemplates Hub\\nLangChain Hub\\nJS/TS Docs\\n\\nüí¨\\nSearch\\n\\n\\nModel I/O\\n\\n\\nPrompts\\n\\n\\nChat models\\n\\n\\nLLMs', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5b4f123e-7c1b-4e6a-81fb-7f858ed075cf', embedding=None, metadata={'url': 'https://js.langchain.com/v0.1/docs/modules/chains/foundational/llm_chain/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chat\\nSearch\\nThis is documentation for LangChain v0.1, which is no longer actively maintained.\\nFor the current stable version, see this version (Latest).\\nOn this page\\nLLM\\nAn LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents. [...] An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\\nGet started\\u200b\\nWe can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM:\\ntip\\nSee this section for general instructions on installing integration packages. [...] import { OpenAI } from \"@langchain/openai\";import { LLMChain } from \"langchain/chains\";import { PromptTemplate } from \"@langchain/core/prompts\";// We can construct an LLMChain from a PromptTemplate and an LLM.const model = new OpenAI({ temperature: 0 });const prompt = PromptTemplate.fromTemplate(  \"What is a good name for a company that makes {product}?\");const chainA = new LLMChain({ llm: model, prompt });// The result is an object with a `text` property.const resA = await chainA.invoke({', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cf8d9599-2ac9-4879-bada-4c1358d5141b', embedding=None, metadata={'url': 'https://scalexi.medium.com/understanding-the-differences-between-llm-chains-and-llm-agent-executors-in-langchain-3f3cf402442f'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"LangChain has emerged as a robust framework for building applications powered by large language models (LLMs). Two fundamental concepts within LangChain are LLM Chains and LLM Agent Executors, both of which leverage tools to enhance the capabilities of LLMs. While they may seem similar at first glance, understanding their differences is crucial for developers aiming to harness LangChain's full potential. [...] Both LLM Chains and LLM Agent Executors offer powerful ways to structure and execute tasks using LangChain, but they are designed for different use cases. Understanding the key differences is essential for choosing the right approach. [...] By selecting the appropriate architecture ‚Äî whether it‚Äôs the simplicity of LLM Chains or the adaptability of LLM Agent Executors ‚Äî you can build more efficient, intelligent, and effective applications using LangChain, tailored to the specific needs of your project.\\n\\nIf you need further information or assistance, feel free to contact ScaleX Innovation at info@scalexi.ai.\\n\\n--\\n\\n--\\n\\n2\\n\\nWritten by ScaleX Innovation\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ed77cbf3-6f24-4672-92fa-a4536f1ae567', embedding=None, metadata={'url': 'https://www.stardog.com/blog/designing-llm-applications-with-knowledge-graphs-and-langchain/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LangChain is described as ‚Äúa framework for developing applications powered by language models‚Äù ‚Äî which is precisely how we use it within Voicebox. Its two central concepts for us are Chain and Vectorstore. [...] Summary\\n\\nLangChain has been a lot of fun to work with in Voicebox. It fit our needs for a framework in Python, allowing us to quickly transition from primitive, low-level code working directly with the LLM and getting every bit of mileage out of f-strings for prompt creation, to something more structured and extensible. [...] Vectorstore is pretty obvious; everyone has one in their stack. The LangChain API is well designed, and it was a trivial task for us to add support for txtai, a vector database with which we‚Äôd already had a lot of experience and tooling. This removed some learning curve hurdles, and we were able to iterate faster to more sophisticated versions of Voicebox‚Äôs core features.\\n\\nA Chain encapsulates the logic of using an LLM to accomplish a specific task. From the LangChain site:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='21aa0bf9-58ea-4542-a82c-97f9d4c251ac', embedding=None, metadata={'url': 'https://www.langchain.com/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Build\\n\\nLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.\\n\\nRun\\n\\nDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.\\n\\nManage\\n\\nLangSmith is a unified agent observability and evals platform to optimize the performance of your\\xa0AI agents - whether they're built with a LangChain framework or not. [...] ## The reference architecture enterprises adopt for success.\\n\\nLangChain‚Äôs suite of products can be used independently or stacked together for multiplicative impact ‚Äì guiding you through building, running, and managing your LLM apps. [...] [](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ce012c99a9683732d528cf_retrieval-video-transcode.mp4)\\n\\n## Build your app with LangChain\\n\\nBuild context-aware, reasoning applications with LangChain‚Äôs flexible framework that leverages your company‚Äôs data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.\\n\\n[Learn more about LangChain](/langchain)\\n\\n## Run at scale with LangGraph\\xa0Platform\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f8a021eb-f623-4c8a-976c-65f18aed2315', embedding=None, metadata={'url': 'https://www.pinecone.io/learn/series/langchain/langchain-prompt-templates/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='question cannot be answered using the information provided answer\\nwith \"I don\\'t know\".\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\nTheir superior performance over smaller models has made them incredibly\\nuseful for developers building NLP enabled applications. These models\\ncan be accessed via Hugging Face\\'s transformers library, via OpenAI\\nusing the openai library, and via Cohere using the cohere library.\\nQuestion: Which libraries and model providers offer LLMs? [...] template = \"\"\"Answer the question based on the context below. If the\\nquestion cannot be answered using the information provided answer\\nwith \"I don\\'t know\".\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\nTheir superior performance over smaller models has made them incredibly\\nuseful for developers building NLP enabled applications. These models\\ncan be accessed via Hugging Face\\'s transformers library, via OpenAI [...] prompt = \"\"\"Answer the question based on the context below. If the\\nquestion cannot be answered using the information provided answer\\nwith \"I don\\'t know\".\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\nTheir superior performance over smaller models has made them incredibly\\nuseful for developers building NLP enabled applications. These models\\ncan be accessed via Hugging Face\\'s transformers library, via OpenAI', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f232dcb0-4a78-4567-aa04-06cec59d9604', embedding=None, metadata={'url': 'https://python.langchain.com/docs/integrations/llms/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"LLMs\\ncaution\\nYou are currently on a page documenting the use of text completion models. Many of the latest and most popular models are chat completion models.\\nUnless you are specifically using more advanced prompting techniques, you are probably looking for this page instead.\\nLLMs are language models that take a string as input and return a string as output.\\ninfo\\nIf you'd like to write your own LLM, see this how-to. If you'd like to contribute an integration, see Contributing integrations. [...] | Yuan2.0 | Yuan2.0 is a new generation Fundamental Large Language Model develope... |\\nEdit this page [...] | Provider | Package |\\n| --- | --- |\\n| AI21LLM | langchain-ai21 |\\n| AnthropicLLM | langchain-anthropic |\\n| AzureOpenAI | langchain-openai |\\n| BedrockLLM | langchain-aws |\\n| CohereLLM | langchain-cohere |\\n| FireworksLLM | langchain-fireworks |\\n| OllamaLLM | langchain-ollama |\\n| OpenAILLM | langchain-openai |\\n| TogetherLLM | langchain-together |\\n| VertexAILLM | langchain-google_vertexai |\\n| NVIDIA | langchain-nvidia |\\nAll LLMs\\u200b\\n| Name | Description |\\n| --- | --- |\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='616cd9a6-0236-4ac1-b856-11965e3bdeda', embedding=None, metadata={'url': 'https://www.youtube.com/watch?v=AjQPRomyd-k'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"LLM Project | End to End Gen AI Project Using LangChain, Google Palm In Ed-Tech Industry \\n codebasics \\n 2910 likes \\n 119522 views \\n 27 Oct 2023 \\n This is an End-to-End LLM project using the langchain framework. We are building a question-and-answer system for a real e-learning company (no toy datasets). If you are new to GenAI and LLM application development, then this langchain tutorial will help you understand how to build the end-to-end application. [...] we are going to build end to endend llm project where we will use Lang chain hugging face streamlet and Google Palm model the use case we are solving is for a real e-learning company we have not used any toy data set here that company is my own company code Basics we have this code basic. platform which has many data related courses with more than 20,000 students now these existing students or a person who wants to buy the course they will have questions related to courses fees Etc and we will [...] this requirement. txt file see this txt file has all these modules which will be using in our project and you can go to this folder and simply run p install r hyph r requirement. txt and it will install all the modules by the way I'm giving all this code check in video description you'll be able to download all this code okay it has this faq.com text file and from this place now I'm going to import Google prom so you'll say from Lang chain. llm import Google pal and then llm object is equal to\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b304a8b2-590f-48d4-8950-dfbbb0e41b20', embedding=None, metadata={'url': 'https://www.reddit.com/r/LangChain/comments/1al25wz/langchain_for_llm/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='You can use pretty much most of the new fully local models. Langchain supports Ollama through their open ai API support so you can use RAG using', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0f98c5e7-43a5-4c5c-9898-a63a5fe8eae6', embedding=None, metadata={'url': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Whether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to the global verbose value,\\naccessible via langchain.globals.get_verbose().\\n\\nCreate LLMChain from LLM and template.\\n\\nllm (BaseLanguageModel)\\n\\ntemplate (str)\\n\\nLLMChain\\n\\nDeprecated since version 0.1.0: Use invoke() instead. It will not be removed until langchain==1.0.\\n\\nExecute the chain. [...] Section Navigation\\n\\nBase packages\\n\\nIntegrations\\n\\nLLMChain#\\n\\nBases: Chain\\n\\nDeprecated since version 0.1.17: Use , `prompt | llm`() instead. It will not be removed until langchain==1.0.\\n\\nChain to run queries against LLMs.\\n\\nThis class is deprecated. See below for an example implementation using\\nLangChain runnables:\\n\\nExample\\n\\nNote\\n\\nLLMChain implements the standard Runnable Interface. üèÉ [...] Call apply and then parse the results.\\n\\ninput_list (List[Dict[str, Any]])\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\nSequence[str | List[str] | Dict[str, str]]\\n\\nCall apredict and then parse the results.\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\nkwargs (Any)\\n\\nstr | List[str] | Dict[str, str]\\n\\nPrepare chain inputs, including adding inputs from memory.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')], is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = AgentRunner(agent_worker)\n",
    "\n",
    "agent.chat(\"Me retorne artigos sobre LLM e LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ab90ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "617de287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 108 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "url = \"../../data/LLM.pdf\"\n",
    "article = SimpleDirectoryReader(input_files=[url]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31d8edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../../data/LLM_2.pdf\"\n",
    "tutorial = SimpleDirectoryReader(input_files=[url]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35d2c4",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4da80298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bc86032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817ba6f2317c40dba05df6ec9b60c8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a0fb4af579421db96e1293bdfa5289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/160k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a469858f968c45c2b121d1de75132b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852fef67e41b4b849b557e012ccbc7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19371787f2dd4a61aae3fecb32c90126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73fc3028d404952913dd131851b34ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d706310b376b43ca9ccf5527c09f720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e7a3ab087b46219559e63e09f3f35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b80ccf762544598338c4ceefde7ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f65c57ac26e415f993bc59fb5f64801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"intfloat/multilingual-e5-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96d95533",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_index = VectorStoreIndex.from_documents(article)\n",
    "tutorial_index = VectorStoreIndex.from_documents(tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbaf1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_index.storage_context.persist(persist_dir=\"embedding/article\")\n",
    "tutorial_index.storage_context.persist(persist_dir=\"embedding/tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb95bd",
   "metadata": {},
   "source": [
    "### Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6288d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.tools import QueryEngineTool,ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fad8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"embedding/article\"\n",
    ")\n",
    "\n",
    "article_index = load_index_from_storage(article_storage_context)\n",
    "\n",
    "tutorial_storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"embedding/tutorial\"\n",
    ")\n",
    "\n",
    "tutorial_index = load_index_from_storage(tutorial_storage_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ce3f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_engine = article_index.as_query_engine(similarity_top_k=3, llm=llm)\n",
    "tutorial_engine = tutorial_index.as_query_engine(similarity_top_k=3, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "129ea0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=article_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"article_engine\",\n",
    "            description=(\n",
    "                \"Fornece informa√ß√µes sobre LLM e LangChain\"\n",
    "                \"Use uma pergunta detalhada em texto simples como entrada para a ferramenta\"\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=tutorial_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"tutorial_engine\",\n",
    "            description=(\n",
    "                \"Fornece informa√ß√µes sobre casos de uso e aplica√ß√µes em LLMs\"\n",
    "                \"Use uma pergunta detalhada em texto simples como entrada para a ferramenta\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7eee6ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    query_engine_tools,\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=True,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "agent_document = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "67760f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Quais as principais aplica√ß√µes que posso construir com LLM e LangChain?\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es que posso construir com LLM e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplica√ß√µes que voc√™ pode construir com LLM (Large Language Models) e LangChain incluem:\n",
      "\n",
      "1. **Cria√ß√£o e aprimoramento de conte√∫do**: Gera√ß√£o autom√°tica de texto, assist√™ncia na reda√ß√£o, tradu√ß√£o autom√°tica, resumo de textos, planejamento e roteiro de conte√∫do, brainstorming e programa√ß√£o.\n",
      "2. **An√°lise e organiza√ß√£o de informa√ß√µes**: An√°lise de sentimento, extra√ß√£o de informa√ß√µes, classifica√ß√£o de textos, revis√£o t√©cnica e outras aplica√ß√µes de processamento de linguagem natural.\n",
      "3. **Intera√ß√£o e automa√ß√£o**: Chatbots, perguntas e respostas, gera√ß√£o de respostas a perguntas com base em um corpus, e outras aplica√ß√µes de intera√ß√£o com usu√°rios.\n",
      "4. **Desenvolvimento de software**: Uso de LLMs para auxiliar programadores, acelerar a gera√ß√£o de c√≥digo e melhorar a qualidade do desenvolvimento de software, como no caso do GitHub Copilot ou do StarCoder.\n",
      "5. **Ferramentas de escrit√≥rio**: Integra√ß√£o de LLMs √†s ferramentas de desenvolvimento de software e de escrit√≥rio, como o Microsoft 365 Copilot e o Google Workspace, para transformar a efici√™ncia e a capacidade das empresas.\n",
      "\n",
      "Al√©m disso, com o LangChain, voc√™ pode construir aplica√ß√µes que combinam LLMs com outras tecnologias, como blockchain, para criar solu√ß√µes inovadoras e seguras para diversas ind√∫strias.\n",
      "=== Calling Function ===\n",
      "Calling function: article_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es que posso construir com LLM e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplica√ß√µes que voc√™ pode construir com LLMs incluem: \n",
      "\n",
      "1. Chatbots e assistentes virtuais para fornecer suporte ao cliente, solu√ß√£o de problemas ou conversas abertas.\n",
      "2. Gera√ß√£o de c√≥digo e depura√ß√£o para fornecer trechos √∫teis de c√≥digo como resposta a solicita√ß√µes escritas em linguagem natural.\n",
      "3. An√°lise de sentimento para ajudar a analisar emo√ß√µes e opini√µes a partir de um texto.\n",
      "4. Classifica√ß√£o e agrupamento de texto para categorizar e classificar grandes volumes de dados.\n",
      "5. Tradu√ß√£o de idiomas para globalizar todo o seu conte√∫do sem horas de trabalho √°rduo.\n",
      "6. Resumo e par√°fraseamento para resumir chamadas ou reuni√µes de clientes de forma eficiente.\n",
      "7. Gera√ß√£o de conte√∫do para desenvolver um esbo√ßo ou gerar um primeiro rascunho a partir de um prompt detalhado.\n",
      "\n",
      "Essas s√£o apenas algumas das muitas aplica√ß√µes poss√≠veis com LLMs, e a escolha da aplica√ß√£o certa depender√° das necessidades espec√≠ficas do seu projeto ou organiza√ß√£o.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es que posso construir com LLM e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplica√ß√µes que voc√™ pode construir com LLM (Large Language Models) e LangChain incluem:\n",
      "\n",
      "1. **Cria√ß√£o e aprimoramento de conte√∫do**: Voc√™ pode usar LLMs para gerar conte√∫do automaticamente, como textos, resumos, tradu√ß√µes e at√© mesmo c√≥digos de programa√ß√£o.\n",
      "2. **An√°lise e organiza√ß√£o de informa√ß√µes**: LLMs podem ser usados para analisar sentimentos, extrair informa√ß√µes espec√≠ficas de documentos grandes, classificar textos em categorias ou temas espec√≠ficos e revisar documentos especializados.\n",
      "3. **Intera√ß√£o e automa√ß√£o**: Voc√™ pode construir chatbots que simulam conversas sobre t√≥picos gerais ou espec√≠ficos, gerar respostas a perguntas com base em um corpus e criar experi√™ncias interativas ricas com entrada n√£o apenas de texto, mas tamb√©m de imagem, √°udio e v√≠deo.\n",
      "4. **Desenvolvimento de software**: LLMs podem ser integrados a ferramentas de desenvolvimento de software, como GitHub Copilot ou StarCoder, para auxiliar os programadores, acelerar a gera√ß√£o de c√≥digo e melhorar a qualidade do desenvolvimento de software.\n",
      "5. **Aplicativos de escrit√≥rio**: Voc√™ pode integrar LLMs a ferramentas de escrit√≥rio, como Microsoft 365 Copilot ou Google Workspace, para transformar a efici√™ncia e a capacidade das empresas.\n",
      "\n",
      "Com LangChain, voc√™ pode criar aplica√ß√µes que combinam LLMs com outras tecnologias, como blockchain, para criar solu√ß√µes inovadoras e seguras para uma variedade de setores. Alguns exemplos incluem:\n",
      "\n",
      "1. **Plataformas de aprendizado**: Voc√™ pode criar plataformas de aprendizado que usam LLMs para gerar conte√∫do personalizado e adaptativo para os usu√°rios.\n",
      "2. **Sistemas de recomenda√ß√£o**: LLMs podem ser usados para criar sistemas de recomenda√ß√£o que sugerem produtos ou servi√ßos com base nas prefer√™ncias e comportamentos dos usu√°rios.\n",
      "3. **Ferramentas de colabora√ß√£o**: Voc√™ pode criar ferramentas de colabora√ß√£o que usam LLMs para ajudar as equipes a trabalhar juntas de forma mais eficiente e eficaz.\n",
      "\n",
      "Essas s√£o apenas algumas das principais aplica√ß√µes que voc√™ pode construir com LLM e LangChain. A combina√ß√£o dessas tecnologias oferece um vasto potencial para criar solu√ß√µes inovadoras e transformadoras em uma variedade de setores.\n",
      "=== Calling Function ===\n",
      "Calling function: article_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es que posso construir com LLM e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplica√ß√µes que voc√™ pode construir com LLMs incluem: \n",
      "\n",
      "1. Chatbots e assistentes virtuais para fornecer suporte ao cliente, solu√ß√£o de problemas ou conversas abertas.\n",
      "2. Gera√ß√£o de c√≥digo e depura√ß√£o para fornecer trechos √∫teis de c√≥digo como resposta a solicita√ß√µes escritas em linguagem natural.\n",
      "3. An√°lise de sentimento para ajudar a analisar emo√ß√µes e opini√µes a partir de um texto.\n",
      "4. Classifica√ß√£o e agrupamento de texto para categorizar e classificar grandes volumes de dados.\n",
      "5. Tradu√ß√£o de idiomas para globalizar todo o seu conte√∫do sem horas de trabalho √°rduo.\n",
      "6. Resumo e par√°fraseamento para resumir chamadas ou reuni√µes de clientes de forma eficiente.\n",
      "7. Gera√ß√£o de conte√∫do para desenvolver um esbo√ßo e criar ideias.\n",
      "\n",
      "Essas s√£o apenas algumas das aplica√ß√µes poss√≠veis, e as op√ß√µes podem variar dependendo das necessidades espec√≠ficas da sua organiza√ß√£o e dos recursos dispon√≠veis.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es que posso construir com LLM e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplica√ß√µes que voc√™ pode construir com LLM (Large Language Models) e LangChain incluem:\n",
      "\n",
      "1. **Cria√ß√£o e aprimoramento de conte√∫do**: Gera√ß√£o autom√°tica de texto, assist√™ncia na reda√ß√£o, tradu√ß√£o autom√°tica, resumo de textos, planejamento e roteiro de conte√∫do, brainstorming e programa√ß√£o.\n",
      "2. **An√°lise e organiza√ß√£o de informa√ß√µes**: An√°lise de sentimento, extra√ß√£o de informa√ß√µes, classifica√ß√£o de textos, revis√£o t√©cnica e outras aplica√ß√µes de processamento de linguagem natural.\n",
      "3. **Intera√ß√£o e automa√ß√£o**: Chatbots, perguntas e respostas, gera√ß√£o de respostas a perguntas com base em um corpus, e outras aplica√ß√µes de intera√ß√£o com usu√°rios.\n",
      "4. **Desenvolvimento de software**: Ferramentas como GitHub Copilot ou StarCoder usam LLM para auxiliar os programadores, acelerando a gera√ß√£o de c√≥digo e melhorando a qualidade do desenvolvimento de software.\n",
      "5. **Aplicativos avan√ßados em diversos setores**: Com a integra√ß√£o do LLM √†s ferramentas de desenvolvimento de software e de escrit√≥rio, √© poss√≠vel criar aplicativos avan√ßados em √°reas como sa√∫de, finan√ßas, educa√ß√£o, entre outras.\n",
      "\n",
      "Al√©m disso, com o LangChain, voc√™ pode construir aplica√ß√µes que integrem LLMs com outras tecnologias, como blockchain, para criar solu√ß√µes inovadoras e seguras. As possibilidades s√£o amplas e dependem da sua criatividade e habilidades em desenvolvimento.\n"
     ]
    }
   ],
   "source": [
    "response = agent_document.chat(\n",
    "    \"Quais as principais aplica√ß√µes que posso construir com LLM e LangChain?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c822ed7c",
   "metadata": {},
   "source": [
    "### Agent ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "876cb6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "295f9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0cc99f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 12d49f0a-c40d-41b2-824f-fc450374a2ac. Step input: Quais s√£o as principais ferramentas usadas em LangChain?\n",
      "\u001b[1;3;38;5;200mThought: O usu√°rio est√° perguntando sobre as principais ferramentas usadas em LangChain. Eu preciso usar a ferramenta article_engine para obter informa√ß√µes sobre as ferramentas usadas em LangChain.\n",
      "Action: article_engine\n",
      "Action Input: {'input': 'Quais s√£o as principais ferramentas usadas em LangChain?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: N√£o h√° men√ß√£o √†s principais ferramentas usadas em LangChain no contexto fornecido. O contexto discute o desenvolvimento e a aplica√ß√£o de grandes modelos de linguagem (LLM), incluindo servi√ßos propriet√°rios e modelos de c√≥digo aberto, mas n√£o fornece informa√ß√µes sobre LangChain ou suas ferramentas.\n",
      "\u001b[0m> Running step ef32f74f-9567-4ee5-ad6a-e7043b63f5a3. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: A ferramenta article_engine n√£o forneceu informa√ß√µes suficientes sobre as principais ferramentas usadas em LangChain. Eu preciso tentar novamente com a ferramenta tutorial_engine para obter mais informa√ß√µes.\n",
      "Action: tutorial_engine\n",
      "Action Input: {'input': 'Quais s√£o as principais ferramentas usadas em LangChain?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: N√£o h√° men√ß√£o √†s principais ferramentas usadas em LangChain no contexto fornecido. O contexto fornece informa√ß√µes sobre LLMs (Modelos de Linguagem Grande), suas aplica√ß√µes, arquiteturas e exemplos de uso, mas n√£o menciona especificamente o LangChain ou suas ferramentas.\n",
      "\u001b[0m> Running step ded00d84-8330-45e6-9013-c643d62b426e. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: As ferramentas article_engine e tutorial_engine n√£o forneceram informa√ß√µes suficientes sobre as principais ferramentas usadas em LangChain. Eu n√£o tenho mais informa√ß√µes para fornecer e devo informar ao usu√°rio que n√£o posso responder √† pergunta.\n",
      "Answer: Infelizmente, n√£o consegui encontrar informa√ß√µes sobre as principais ferramentas usadas em LangChain com as ferramentas dispon√≠veis. Se voc√™ tiver mais contexto ou detalhes, posso tentar ajudar novamente.\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='Infelizmente, n√£o consegui encontrar informa√ß√µes sobre as principais ferramentas usadas em LangChain com as ferramentas dispon√≠veis. Se voc√™ tiver mais contexto ou detalhes, posso tentar ajudar novamente.', sources=[ToolOutput(content='N√£o h√° men√ß√£o √†s principais ferramentas usadas em LangChain no contexto fornecido. O contexto discute o desenvolvimento e a aplica√ß√£o de grandes modelos de linguagem (LLM), incluindo servi√ßos propriet√°rios e modelos de c√≥digo aberto, mas n√£o fornece informa√ß√µes sobre LangChain ou suas ferramentas.', tool_name='article_engine', raw_input={'input': 'Quais s√£o as principais ferramentas usadas em LangChain?'}, raw_output=Response(response='N√£o h√° men√ß√£o √†s principais ferramentas usadas em LangChain no contexto fornecido. O contexto discute o desenvolvimento e a aplica√ß√£o de grandes modelos de linguagem (LLM), incluindo servi√ßos propriet√°rios e modelos de c√≥digo aberto, mas n√£o fornece informa√ß√µes sobre LangChain ou suas ferramentas.', source_nodes=[NodeWithScore(node=TextNode(id_='4d09a829-3eee-4ce1-b6e6-241ea059647c', embedding=None, metadata={'page_label': '3', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='958de098-935b-4080-b0f9-7ce1d2fadccc', node_type='4', metadata={'page_label': '3', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='9433b67fb06c58ed701220217278d28f6c75fb118f42093be7f17572b4e50b22')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='3 \\n \\n  \\n      Breve hist√≥rico e resumo do desenvolvimento dos LLMs  D√©cadas de 1950 a 1990 Foram feitas tentativas iniciais para criar regras r√≠gidas para as linguagens e seguir passos l√≥gicos para realizar tarefas como traduzir frases de um idioma para outro. Embora esse m√©todo funcionasse em alguns casos, estava limitado a tarefas bem definidas das quais o sistema tinha conhecimento.  D√©cada de 1990 Os modelos de linguagem come√ßaram a evoluir para modelos estat√≠sticos, e os padr√µes lingu√≠sticos come√ßaram a ser analisados, mas projetos em larga escala eram limitados pela capacidade de processamento de dados.  Anos 2000 Os avan√ßos em machine learning aumentaram a complexidade dos modelos de linguagem, e a ampla ado√ß√£o da internet forneceu uma grande quantidade de dados de treinamento.  2012 Os avan√ßos em arquiteturas de deep learning e conjuntos de dados maiores levaram ao desenvolvimento do GPT (Transformadores Pr√©-treinados Generativos). \\n2018 O Google apresentou o BERT (Bidirectional Encoder Representations from Transformers), que foi um grande salto na arquitetura e abriu caminho para futuros grandes modelos de linguagem.  2020 A OpenAI lan√ßou o GPT-3, que se tornou o maior modelo com 175 bilh√µes de par√¢metros e estabeleceu um novo referencial de desempenho para tarefas relacionadas √† linguagem.  2022 O ChatGPT foi lan√ßado, transformando o GPT-3 e modelos semelhantes em um servi√ßo amplamente acess√≠vel aos usu√°rios por meio de uma interface web, o que iniciou um aumento significativo na conscientiza√ß√£o p√∫blica sobre LLMs e IA generativa.  2023 Os LLMs de c√≥digo aberto come√ßam a apresentar resultados cada vez mais impressionantes, com lan√ßamentos como Dolly 2.0, LLaMA, Alpaca e Vicuna. O GPT-4 tamb√©m √© lan√ßado, estabelecendo um novo referencial tanto em tamanho de par√¢metros quanto em desempenho.', mimetype='text/plain', start_char_idx=0, end_char_idx=1829, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8225544256844811), NodeWithScore(node=TextNode(id_='1e15d660-e281-4b6f-bfc3-d38075e52259', embedding=None, metadata={'page_label': '7', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4b8fd5b3-8a60-4c02-90b9-04c5709182bd', node_type='4', metadata={'page_label': '7', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='5bcaddc1ec65bb45ae328519d5238e15518a0a51d7eaf9948875be845faf24c1')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Um guia compacto sobre grandes modelos de linguagem (LLM) 7  \\n \\n        Atualmente, requer um pouco mais de esfor√ßo para pegar um modelo de c√≥digo aberto e come√ßar a us√°-lo, mas o progresso est√° ocorrendo muito rapidamente para torn√°-los mais acess√≠veis aos usu√°rios. Na Databricks, por exemplo, fizemos melhorias em frameworks de c√≥digo aberto como o MLflow para tornar muito f√°cil para algu√©m com um pouco de experi√™ncia em Python pegar qualquer modelo transformador da Hugging Face e us√°-lo como um objeto Python. Muitas vezes, voc√™ pode encontrar um modelo de c√≥digo aberto que resolve seu problema espec√≠fico e que √© v√°rias ordens de grandeza menor que o ChatGPT, permitindo que voc√™ traga o modelo para seu ambiente e hospede-o voc√™ mesmo. Isso significa que voc√™ pode manter os dados sob seu controle para preocupa√ß√µes com privacidade e governan√ßa, al√©m de gerenciar seus custos. Outra grande vantagem de usar modelos de c√≥digo aberto √© a capacidade de ajust√°-los aos seus pr√≥prios dados. Como voc√™ n√£o est√° lidando com uma caixa preta de um servi√ßo propriet√°rio, existem t√©cnicas que permitem pegar modelos de c√≥digo aberto e trein√°-los com seus dados espec√≠ficos, melhorando significativamente o desempenho deles em seu dom√≠nio espec√≠fico. Acreditamos que o futuro dos modelos de linguagem seguir√° nessa dire√ß√£o, √† medida que mais organiza√ß√µes desejem ter controle total e compreens√£o de seus LLMs. \\nConclus√£o e diretrizes gerais Em √∫ltima an√°lise, cada organiza√ß√£o ter√° desafios √∫nicos a superar, e n√£o existe uma abordagem √∫nica para os LLMs. √Ä medida que o mundo se torna mais orientado a dados, tudo, incluindo os LLMs, depender√° de uma base s√≥lida de dados. Os LLMs s√£o ferramentas incr√≠veis, mas devem ser usados e implementados sobre essa base s√≥lida de dados. A Databricks oferece tanto essa base s√≥lida de dados quanto as ferramentas integradas para permitir que voc√™ use e ajuste os LLMs no seu dom√≠nio.', mimetype='text/plain', start_char_idx=0, end_char_idx=1922, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8208696136295812), NodeWithScore(node=TextNode(id_='38cbec41-d702-4eff-8099-5e7fdc402387', embedding=None, metadata={'page_label': '6', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f37f42b0-a2fa-438a-8c2e-166cbee93d12', node_type='4', metadata={'page_label': '6', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='bb55238cd5509440a52373544201167d418586bf98429e728ad5c7923570aa81')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='6 \\n \\n  \\n      PARTE 3 \\nAplica√ß√£o de grandes modelos de linguagem     Existem alguns caminhos que voc√™ pode seguir ao procurar aplicar grandes modelos de linguagem para seu caso de uso espec√≠fico. Em termos gerais, voc√™ pode dividi-los em duas categorias, mas h√° alguma sobreposi√ß√£o entre elas. Vamos abordar brevemente as vantagens e desvantagens de cada uma e em quais cen√°rios cada uma se encaixa melhor.   Servi√ßos propriet√°rios Como o primeiro servi√ßo amplamente dispon√≠vel alimentado por LLM, o ChatGPT da OpenAI foi o catalisador explosivo que trouxe os LLMs para o mainstream. O ChatGPT fornece uma interface de usu√°rio (ou API) em que os usu√°rios podem enviar prompts para muitos modelos (GPT-3.5, GPT-4 e outros) e geralmente obter uma resposta r√°pida. Eles est√£o entre os modelos de maior desempenho, treinados em conjuntos de dados enormes, e s√£o capazes de realizar tarefas extremamente complexas tanto do ponto de vista t√©cnico, como gera√ß√£o de c√≥digo, quanto do ponto de vista criativo, como escrever poesia em um estilo espec√≠fico. A desvantagem desses servi√ßos √© a quantidade absolutamente enorme de recursos computacionais necess√°rios n√£o apenas para trein√°-los (a OpenAI afirmou que o GPT-4 custou mais de US$ 100 milh√µes para desenvolver), mas tamb√©m para fornecer as respostas. Por esse motivo, esses modelos extremamente grandes provavelmente sempre estar√£o sob o controle de organiza√ß√µes \\ne exigir√£o que voc√™ envie seus dados para seus servidores a fim de interagir com seus modelos de linguagem. Isso levanta preocupa√ß√µes com privacidade e seguran√ßa, e tamb√©m sujeita os usu√°rios a modelos ‚Äúcaixa preta‚Äù, sobre cujos treinamentos e limites eles n√£o t√™m controle. Al√©m disso, devido aos recursos computacionais necess√°rios, esses servi√ßos n√£o s√£o gratuitos al√©m de um uso muito limitado, ent√£o o custo se torna um fator ao aplic√°-los em grande escala. Resumindo: servi√ßos propriet√°rios s√£o √≥timos para usar se voc√™ tiver tarefas muito complexas, tiver disposi√ß√£o para compartilhar seus dados com terceiros e quiser incorrer em custos ao operar em escala significativa.   Modelos de c√≥digo aberto A outra op√ß√£o para modelos de linguagem √© recorrer √† comunidade de c√≥digo aberto, onde houve um crescimento igualmente explosivo nos √∫ltimos anos. Comunidades como a Hugging Face re√∫nem centenas de milhares de modelos de contribuidores que podem ajudar a resolver muitos casos de uso espec√≠ficos, como gera√ß√£o de texto, resumo e classifica√ß√£o. A comunidade de c√≥digo aberto est√° rapidamente alcan√ßando o desempenho dos modelos propriet√°rios, mas ainda n√£o conseguiu igualar o desempenho de algo como o GPT-4.', mimetype='text/plain', start_char_idx=0, end_char_idx=2626, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8180327369788885)], metadata={'4d09a829-3eee-4ce1-b6e6-241ea059647c': {'page_label': '3', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, '1e15d660-e281-4b6f-bfc3-d38075e52259': {'page_label': '7', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, '38cbec41-d702-4eff-8099-5e7fdc402387': {'page_label': '6', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}}), is_error=False), ToolOutput(content='N√£o h√° men√ß√£o √†s principais ferramentas usadas em LangChain no contexto fornecido. O contexto fornece informa√ß√µes sobre LLMs (Modelos de Linguagem Grande), suas aplica√ß√µes, arquiteturas e exemplos de uso, mas n√£o menciona especificamente o LangChain ou suas ferramentas.', tool_name='tutorial_engine', raw_input={'input': 'Quais s√£o as principais ferramentas usadas em LangChain?'}, raw_output=Response(response='N√£o h√° men√ß√£o √†s principais ferramentas usadas em LangChain no contexto fornecido. O contexto fornece informa√ß√µes sobre LLMs (Modelos de Linguagem Grande), suas aplica√ß√µes, arquiteturas e exemplos de uso, mas n√£o menciona especificamente o LangChain ou suas ferramentas.', source_nodes=[NodeWithScore(node=TextNode(id_='c3745170-f446-48be-81f5-e977731f6a49', embedding=None, metadata={'page_label': '3', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d45043cd-1d20-4559-9bb4-c6bf9f6f0f33', node_type='4', metadata={'page_label': '3', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='7e418432cd6caf3b88ad7ca49cc6ed1f5a14b0295e33caa6286a8f4d35f9a4fc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='307696df-3841-49a1-ac19-9e2519f335ab', node_type='1', metadata={'page_label': '3', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='745bf1c2291e4dd8708967c9c8347a3198f8a79cc8b31cba800f6bb6b7acd531')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Estados Unidos\\nMicrosoft Orca Concentra-se na cria√ß√£o de dados sint√©ticos e em recursos de \\nracioc√≠nio aprimorados.\\nEstados Unidos\\nAnthropic Claude Reconhecido por seu amplo conhecimento geral e recursos \\nmultil√≠ngues.\\nEstados Unidos\\nGoogle Gemini, Gemma, BERT Pioneira no processamento de idiomas com modelos que suportam \\nv√°rios tipos de dados.\\nEstados Unidos\\nMeta AI Llama Conhecida pela efici√™ncia e pelo acesso democratizado, com foco no \\nalto desempenho com computa√ß√£o reduzida.\\nEstados Unidos\\nLMSYS Vicuna Ajustado para funcionalidades de chatbot, oferecendo uma \\nabordagem exclusiva para intera√ß√µes de conversa√ß√£o.\\nEstados Unidos\\nCohere Command-nightly Especializada em tempos de resposta r√°pidos e pesquisa sem√¢ntica \\nem mais de 100 idiomas.\\nCanad√°\\nMistral AI Mistral, Mixtral Enfatiza modelos menores, mas poderosos, operando localmente com \\nm√©tricas de desempenho s√≥lidas.\\nFrancia\\nClibrain LINCE Adaptado para o idioma espanhol, com foco em nuances lingu√≠sticas e \\ncompreens√£o de qualidade.\\nEspa√±a\\nTechnology \\nInnovation Institute\\nFalcon Fornece modelos de IA de c√≥digo aberto altamente eficientes e \\ndimension√°veis com suporte multil√≠ngue.\\nEmiratos √Årabes Unidos\\nAleph Alpha Luminous Not√°vel por sua abordagem multimodal e desempenho competitivo \\nnas principais tarefas de IA.\\nAlemania \\nSenseTime SenseNova Uma s√©rie de modelos e aplicativos de IA generativa que fazem uso da \\nplataforma de pesquisa e desenvolvimento da AGI e integram LLMs \\ncom sistemas de computa√ß√£o em larga escala (SenseCore, com 5.000 \\npetaflops). \\nHong Kong', mimetype='text/plain', start_char_idx=2851, end_char_idx=4393, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8213836173844439), NodeWithScore(node=TextNode(id_='e4232058-b208-477c-b22c-7b3e2a8a558d', embedding=None, metadata={'page_label': '4', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9c39d1b2-c70c-46be-93a0-0487312d6597', node_type='4', metadata={'page_label': '4', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='15c4b12cf049edc317f599169556f77b8c5c35eb4ed4803335f62e1b7adb58a9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='17\\ninerentes. Nesse per√≠odo, tamb√©m houve uma preocupa√ß√£o \\ncrescente com as considera√ß√µes e os desafios √©ticos \\napresentados pelo desenvolvimento e uso de LLMs e, como \\nconsequ√™ncia, um avan√ßo na regulamenta√ß√£o da IA e da IA \\ngenerativa em todo o mundo. \\nA prolifera√ß√£o de LLMs de c√≥digo aberto foi um marco na \\ndemocratiza√ß√£o da tecnologia de IA. Come√ßando com o Llama e \\ncontinuando com Vicuna, Falcon, Mistral, Gemma e outros, os \\nLLMs de c√≥digo aberto democratizaram o acesso √† tecnologia \\nde ponta de processamento de linguagem e permitiram que \\npesquisadores, desenvolvedores e amadores experimentassem, \\npersonalizassem e implantassem solu√ß√µes de IA com um \\ninvestimento inicial m√≠nimo. A disponibilidade desses modelos \\npromoveu uma colabora√ß√£o sem precedentes na comunidade \\nde IA, estimulando a inova√ß√£o e facilitando a cria√ß√£o de \\naplicativos avan√ßados em diversos setores. \\nPor fim, a integra√ß√£o do LLM √†s ferramentas de \\ndesenvolvimento de software e de escrit√≥rio est√° \\ntransformando a efici√™ncia e a capacidade das empresas. A \\nMicrosoft integrou o LLM em seu pacote Office com o nome \\nMicrosoft 365 Copilot, enquanto o Google fez o mesmo no \\nGoogle Workspace. Ao mesmo tempo, ferramentas como o \\nGitHub Copilot ou o StarCoder usam LLM para auxiliar os \\nprogramadores, acelerando a gera√ß√£o de c√≥digo e melhorando \\na qualidade do desenvolvimento de software. \\n \\nTipologias de LLM \\nOs LLMs progrediram al√©m da simples previs√£o de texto e se \\ntornaram aplicativos sofisticados em v√°rios dom√≠nios, \\narquiteturas e modalidades. Esta se√ß√£o apresenta uma \\ncategoriza√ß√£o dos LLMs de acordo com v√°rios crit√©rios. \\nPor arquitetura \\n4 LLMs baseados em redes neurais recorrentes (RNNs): \\nesses modelos processam o texto sequencialmente, \\nanalisando o impacto de cada palavra sobre a pr√≥xima, e \\nusam arquiteturas recorrentes, como mem√≥ria de longo \\nprazo (LSTM) ou unidades de passagem recorrentes (GRU), \\npara processar dados sequenciais. Embora n√£o sejam t√£o \\neficientes quanto os transformers para sequ√™ncias longas, os \\nRNNs s√£o √∫teis para tarefas em que a compreens√£o da \\nordem das palavras √© crucial, como na tradu√ß√£o autom√°tica. \\nExemplos s√£o o ELMo (Embeddings from Language Models) \\ne o ULMFiT (Universal Language Model Fine-tuning). \\n4 LLMs baseados en transformers: essa √© a arquitetura \\ndominante para LLMs atualmente. Eles usam transformers \\npara analisar as rela√ß√µes entre as palavras em uma frase. Isso \\npermite que eles capturem estruturas gramaticais \\ncomplexas e depend√™ncias de palavras com longa dist√¢ncia. \\nA maioria dos LLMs, como GPT, Claude e Gemini, pertence a \\nessa categoria. \\nPor componente \\n4 Codificadores (Encoders): s√£o modelos projetados para \\nentender (codificar) as informa√ß√µes de entrada. Eles \\ntransformam o texto em uma representa√ß√£o vetorial, \\ncapturando seu significado sem√¢ntico. Os encoders s√£o \\nfundamentais em tarefas como a compreens√£o e a', mimetype='text/plain', start_char_idx=0, end_char_idx=2887, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8212808486997736), NodeWithScore(node=TextNode(id_='6e5a57e1-8b38-4ffa-9c17-f38566cff29c', embedding=None, metadata={'page_label': '6', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3f4c349a-d20b-497c-8ffe-8a99519cd0fc', node_type='4', metadata={'page_label': '6', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='2981185eec61893f3e8ca30abe00bce41b789314d5d4fa9a536989147b4c6ba5'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8cfc1a4e-d2e7-476a-8154-43496913601c', node_type='1', metadata={'page_label': '6', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='04045dafb5fbd29795d87a6371ef24ed79be351decf81a05b8d2711f1e8ea0bf')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Essas \\nferramentas utilizam como insumos as conclus√µes do auditor, \\num banco de dados de relat√≥rios anteriores e um banco de \\ndados de regulamentos internos e externos aplic√°veis. A partir \\ndessas informa√ß√µes, os LLMs geram um rascunho avan√ßado do \\nrelat√≥rio de auditoria, adotando o tom, o vocabul√°rio e o estilo \\ndos auditores humanos e citando adequadamente os relat√≥rios \\nanteriores e as regulamenta√ß√µes relevantes. Isso permite que os \\nauditores economizem muito tempo em tarefas de reda√ß√£o e se \\nconcentrem em atividades de maior valor agregado. \\nEsses exemplos ilustram como os LLMs est√£o criando valor real em \\numa variedade de fun√ß√µes de neg√≥cios, desde a otimiza√ß√£o de \\nprocessos internos at√© a melhoria da experi√™ncia do cliente. Embora \\no n√∫mero de casos de uso em produ√ß√£o seja atualmente limitado, \\nespera-se que essa tend√™ncia se acelere muito rapidamente em um \\nfuturo pr√≥ximo, √† medida que os LLMs continuem a evoluir e os \\ndesafios relacionados √† privacidade e √† seguran√ßa dos dados sejam \\ntratados de forma eficaz.', mimetype='text/plain', start_char_idx=3072, end_char_idx=4105, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8195118718072266)], metadata={'c3745170-f446-48be-81f5-e977731f6a49': {'page_label': '3', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, 'e4232058-b208-477c-b22c-7b3e2a8a558d': {'page_label': '4', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, '6e5a57e1-8b38-4ffa-9c17-f38566cff29c': {'page_label': '6', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}}), is_error=False)], source_nodes=[NodeWithScore(node=TextNode(id_='4d09a829-3eee-4ce1-b6e6-241ea059647c', embedding=None, metadata={'page_label': '3', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='958de098-935b-4080-b0f9-7ce1d2fadccc', node_type='4', metadata={'page_label': '3', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='9433b67fb06c58ed701220217278d28f6c75fb118f42093be7f17572b4e50b22')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='3 \\n \\n  \\n      Breve hist√≥rico e resumo do desenvolvimento dos LLMs  D√©cadas de 1950 a 1990 Foram feitas tentativas iniciais para criar regras r√≠gidas para as linguagens e seguir passos l√≥gicos para realizar tarefas como traduzir frases de um idioma para outro. Embora esse m√©todo funcionasse em alguns casos, estava limitado a tarefas bem definidas das quais o sistema tinha conhecimento.  D√©cada de 1990 Os modelos de linguagem come√ßaram a evoluir para modelos estat√≠sticos, e os padr√µes lingu√≠sticos come√ßaram a ser analisados, mas projetos em larga escala eram limitados pela capacidade de processamento de dados.  Anos 2000 Os avan√ßos em machine learning aumentaram a complexidade dos modelos de linguagem, e a ampla ado√ß√£o da internet forneceu uma grande quantidade de dados de treinamento.  2012 Os avan√ßos em arquiteturas de deep learning e conjuntos de dados maiores levaram ao desenvolvimento do GPT (Transformadores Pr√©-treinados Generativos). \\n2018 O Google apresentou o BERT (Bidirectional Encoder Representations from Transformers), que foi um grande salto na arquitetura e abriu caminho para futuros grandes modelos de linguagem.  2020 A OpenAI lan√ßou o GPT-3, que se tornou o maior modelo com 175 bilh√µes de par√¢metros e estabeleceu um novo referencial de desempenho para tarefas relacionadas √† linguagem.  2022 O ChatGPT foi lan√ßado, transformando o GPT-3 e modelos semelhantes em um servi√ßo amplamente acess√≠vel aos usu√°rios por meio de uma interface web, o que iniciou um aumento significativo na conscientiza√ß√£o p√∫blica sobre LLMs e IA generativa.  2023 Os LLMs de c√≥digo aberto come√ßam a apresentar resultados cada vez mais impressionantes, com lan√ßamentos como Dolly 2.0, LLaMA, Alpaca e Vicuna. O GPT-4 tamb√©m √© lan√ßado, estabelecendo um novo referencial tanto em tamanho de par√¢metros quanto em desempenho.', mimetype='text/plain', start_char_idx=0, end_char_idx=1829, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8225544256844811), NodeWithScore(node=TextNode(id_='1e15d660-e281-4b6f-bfc3-d38075e52259', embedding=None, metadata={'page_label': '7', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4b8fd5b3-8a60-4c02-90b9-04c5709182bd', node_type='4', metadata={'page_label': '7', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='5bcaddc1ec65bb45ae328519d5238e15518a0a51d7eaf9948875be845faf24c1')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Um guia compacto sobre grandes modelos de linguagem (LLM) 7  \\n \\n        Atualmente, requer um pouco mais de esfor√ßo para pegar um modelo de c√≥digo aberto e come√ßar a us√°-lo, mas o progresso est√° ocorrendo muito rapidamente para torn√°-los mais acess√≠veis aos usu√°rios. Na Databricks, por exemplo, fizemos melhorias em frameworks de c√≥digo aberto como o MLflow para tornar muito f√°cil para algu√©m com um pouco de experi√™ncia em Python pegar qualquer modelo transformador da Hugging Face e us√°-lo como um objeto Python. Muitas vezes, voc√™ pode encontrar um modelo de c√≥digo aberto que resolve seu problema espec√≠fico e que √© v√°rias ordens de grandeza menor que o ChatGPT, permitindo que voc√™ traga o modelo para seu ambiente e hospede-o voc√™ mesmo. Isso significa que voc√™ pode manter os dados sob seu controle para preocupa√ß√µes com privacidade e governan√ßa, al√©m de gerenciar seus custos. Outra grande vantagem de usar modelos de c√≥digo aberto √© a capacidade de ajust√°-los aos seus pr√≥prios dados. Como voc√™ n√£o est√° lidando com uma caixa preta de um servi√ßo propriet√°rio, existem t√©cnicas que permitem pegar modelos de c√≥digo aberto e trein√°-los com seus dados espec√≠ficos, melhorando significativamente o desempenho deles em seu dom√≠nio espec√≠fico. Acreditamos que o futuro dos modelos de linguagem seguir√° nessa dire√ß√£o, √† medida que mais organiza√ß√µes desejem ter controle total e compreens√£o de seus LLMs. \\nConclus√£o e diretrizes gerais Em √∫ltima an√°lise, cada organiza√ß√£o ter√° desafios √∫nicos a superar, e n√£o existe uma abordagem √∫nica para os LLMs. √Ä medida que o mundo se torna mais orientado a dados, tudo, incluindo os LLMs, depender√° de uma base s√≥lida de dados. Os LLMs s√£o ferramentas incr√≠veis, mas devem ser usados e implementados sobre essa base s√≥lida de dados. A Databricks oferece tanto essa base s√≥lida de dados quanto as ferramentas integradas para permitir que voc√™ use e ajuste os LLMs no seu dom√≠nio.', mimetype='text/plain', start_char_idx=0, end_char_idx=1922, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8208696136295812), NodeWithScore(node=TextNode(id_='38cbec41-d702-4eff-8099-5e7fdc402387', embedding=None, metadata={'page_label': '6', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f37f42b0-a2fa-438a-8c2e-166cbee93d12', node_type='4', metadata={'page_label': '6', 'file_name': 'LLM.pdf', 'file_path': '../../data/LLM.pdf', 'file_type': 'application/pdf', 'file_size': 1602946, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='bb55238cd5509440a52373544201167d418586bf98429e728ad5c7923570aa81')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='6 \\n \\n  \\n      PARTE 3 \\nAplica√ß√£o de grandes modelos de linguagem     Existem alguns caminhos que voc√™ pode seguir ao procurar aplicar grandes modelos de linguagem para seu caso de uso espec√≠fico. Em termos gerais, voc√™ pode dividi-los em duas categorias, mas h√° alguma sobreposi√ß√£o entre elas. Vamos abordar brevemente as vantagens e desvantagens de cada uma e em quais cen√°rios cada uma se encaixa melhor.   Servi√ßos propriet√°rios Como o primeiro servi√ßo amplamente dispon√≠vel alimentado por LLM, o ChatGPT da OpenAI foi o catalisador explosivo que trouxe os LLMs para o mainstream. O ChatGPT fornece uma interface de usu√°rio (ou API) em que os usu√°rios podem enviar prompts para muitos modelos (GPT-3.5, GPT-4 e outros) e geralmente obter uma resposta r√°pida. Eles est√£o entre os modelos de maior desempenho, treinados em conjuntos de dados enormes, e s√£o capazes de realizar tarefas extremamente complexas tanto do ponto de vista t√©cnico, como gera√ß√£o de c√≥digo, quanto do ponto de vista criativo, como escrever poesia em um estilo espec√≠fico. A desvantagem desses servi√ßos √© a quantidade absolutamente enorme de recursos computacionais necess√°rios n√£o apenas para trein√°-los (a OpenAI afirmou que o GPT-4 custou mais de US$ 100 milh√µes para desenvolver), mas tamb√©m para fornecer as respostas. Por esse motivo, esses modelos extremamente grandes provavelmente sempre estar√£o sob o controle de organiza√ß√µes \\ne exigir√£o que voc√™ envie seus dados para seus servidores a fim de interagir com seus modelos de linguagem. Isso levanta preocupa√ß√µes com privacidade e seguran√ßa, e tamb√©m sujeita os usu√°rios a modelos ‚Äúcaixa preta‚Äù, sobre cujos treinamentos e limites eles n√£o t√™m controle. Al√©m disso, devido aos recursos computacionais necess√°rios, esses servi√ßos n√£o s√£o gratuitos al√©m de um uso muito limitado, ent√£o o custo se torna um fator ao aplic√°-los em grande escala. Resumindo: servi√ßos propriet√°rios s√£o √≥timos para usar se voc√™ tiver tarefas muito complexas, tiver disposi√ß√£o para compartilhar seus dados com terceiros e quiser incorrer em custos ao operar em escala significativa.   Modelos de c√≥digo aberto A outra op√ß√£o para modelos de linguagem √© recorrer √† comunidade de c√≥digo aberto, onde houve um crescimento igualmente explosivo nos √∫ltimos anos. Comunidades como a Hugging Face re√∫nem centenas de milhares de modelos de contribuidores que podem ajudar a resolver muitos casos de uso espec√≠ficos, como gera√ß√£o de texto, resumo e classifica√ß√£o. A comunidade de c√≥digo aberto est√° rapidamente alcan√ßando o desempenho dos modelos propriet√°rios, mas ainda n√£o conseguiu igualar o desempenho de algo como o GPT-4.', mimetype='text/plain', start_char_idx=0, end_char_idx=2626, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8180327369788885), NodeWithScore(node=TextNode(id_='c3745170-f446-48be-81f5-e977731f6a49', embedding=None, metadata={'page_label': '3', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d45043cd-1d20-4559-9bb4-c6bf9f6f0f33', node_type='4', metadata={'page_label': '3', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='7e418432cd6caf3b88ad7ca49cc6ed1f5a14b0295e33caa6286a8f4d35f9a4fc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='307696df-3841-49a1-ac19-9e2519f335ab', node_type='1', metadata={'page_label': '3', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='745bf1c2291e4dd8708967c9c8347a3198f8a79cc8b31cba800f6bb6b7acd531')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Estados Unidos\\nMicrosoft Orca Concentra-se na cria√ß√£o de dados sint√©ticos e em recursos de \\nracioc√≠nio aprimorados.\\nEstados Unidos\\nAnthropic Claude Reconhecido por seu amplo conhecimento geral e recursos \\nmultil√≠ngues.\\nEstados Unidos\\nGoogle Gemini, Gemma, BERT Pioneira no processamento de idiomas com modelos que suportam \\nv√°rios tipos de dados.\\nEstados Unidos\\nMeta AI Llama Conhecida pela efici√™ncia e pelo acesso democratizado, com foco no \\nalto desempenho com computa√ß√£o reduzida.\\nEstados Unidos\\nLMSYS Vicuna Ajustado para funcionalidades de chatbot, oferecendo uma \\nabordagem exclusiva para intera√ß√µes de conversa√ß√£o.\\nEstados Unidos\\nCohere Command-nightly Especializada em tempos de resposta r√°pidos e pesquisa sem√¢ntica \\nem mais de 100 idiomas.\\nCanad√°\\nMistral AI Mistral, Mixtral Enfatiza modelos menores, mas poderosos, operando localmente com \\nm√©tricas de desempenho s√≥lidas.\\nFrancia\\nClibrain LINCE Adaptado para o idioma espanhol, com foco em nuances lingu√≠sticas e \\ncompreens√£o de qualidade.\\nEspa√±a\\nTechnology \\nInnovation Institute\\nFalcon Fornece modelos de IA de c√≥digo aberto altamente eficientes e \\ndimension√°veis com suporte multil√≠ngue.\\nEmiratos √Årabes Unidos\\nAleph Alpha Luminous Not√°vel por sua abordagem multimodal e desempenho competitivo \\nnas principais tarefas de IA.\\nAlemania \\nSenseTime SenseNova Uma s√©rie de modelos e aplicativos de IA generativa que fazem uso da \\nplataforma de pesquisa e desenvolvimento da AGI e integram LLMs \\ncom sistemas de computa√ß√£o em larga escala (SenseCore, com 5.000 \\npetaflops). \\nHong Kong', mimetype='text/plain', start_char_idx=2851, end_char_idx=4393, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8213836173844439), NodeWithScore(node=TextNode(id_='e4232058-b208-477c-b22c-7b3e2a8a558d', embedding=None, metadata={'page_label': '4', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9c39d1b2-c70c-46be-93a0-0487312d6597', node_type='4', metadata={'page_label': '4', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='15c4b12cf049edc317f599169556f77b8c5c35eb4ed4803335f62e1b7adb58a9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='17\\ninerentes. Nesse per√≠odo, tamb√©m houve uma preocupa√ß√£o \\ncrescente com as considera√ß√µes e os desafios √©ticos \\napresentados pelo desenvolvimento e uso de LLMs e, como \\nconsequ√™ncia, um avan√ßo na regulamenta√ß√£o da IA e da IA \\ngenerativa em todo o mundo. \\nA prolifera√ß√£o de LLMs de c√≥digo aberto foi um marco na \\ndemocratiza√ß√£o da tecnologia de IA. Come√ßando com o Llama e \\ncontinuando com Vicuna, Falcon, Mistral, Gemma e outros, os \\nLLMs de c√≥digo aberto democratizaram o acesso √† tecnologia \\nde ponta de processamento de linguagem e permitiram que \\npesquisadores, desenvolvedores e amadores experimentassem, \\npersonalizassem e implantassem solu√ß√µes de IA com um \\ninvestimento inicial m√≠nimo. A disponibilidade desses modelos \\npromoveu uma colabora√ß√£o sem precedentes na comunidade \\nde IA, estimulando a inova√ß√£o e facilitando a cria√ß√£o de \\naplicativos avan√ßados em diversos setores. \\nPor fim, a integra√ß√£o do LLM √†s ferramentas de \\ndesenvolvimento de software e de escrit√≥rio est√° \\ntransformando a efici√™ncia e a capacidade das empresas. A \\nMicrosoft integrou o LLM em seu pacote Office com o nome \\nMicrosoft 365 Copilot, enquanto o Google fez o mesmo no \\nGoogle Workspace. Ao mesmo tempo, ferramentas como o \\nGitHub Copilot ou o StarCoder usam LLM para auxiliar os \\nprogramadores, acelerando a gera√ß√£o de c√≥digo e melhorando \\na qualidade do desenvolvimento de software. \\n \\nTipologias de LLM \\nOs LLMs progrediram al√©m da simples previs√£o de texto e se \\ntornaram aplicativos sofisticados em v√°rios dom√≠nios, \\narquiteturas e modalidades. Esta se√ß√£o apresenta uma \\ncategoriza√ß√£o dos LLMs de acordo com v√°rios crit√©rios. \\nPor arquitetura \\n4 LLMs baseados em redes neurais recorrentes (RNNs): \\nesses modelos processam o texto sequencialmente, \\nanalisando o impacto de cada palavra sobre a pr√≥xima, e \\nusam arquiteturas recorrentes, como mem√≥ria de longo \\nprazo (LSTM) ou unidades de passagem recorrentes (GRU), \\npara processar dados sequenciais. Embora n√£o sejam t√£o \\neficientes quanto os transformers para sequ√™ncias longas, os \\nRNNs s√£o √∫teis para tarefas em que a compreens√£o da \\nordem das palavras √© crucial, como na tradu√ß√£o autom√°tica. \\nExemplos s√£o o ELMo (Embeddings from Language Models) \\ne o ULMFiT (Universal Language Model Fine-tuning). \\n4 LLMs baseados en transformers: essa √© a arquitetura \\ndominante para LLMs atualmente. Eles usam transformers \\npara analisar as rela√ß√µes entre as palavras em uma frase. Isso \\npermite que eles capturem estruturas gramaticais \\ncomplexas e depend√™ncias de palavras com longa dist√¢ncia. \\nA maioria dos LLMs, como GPT, Claude e Gemini, pertence a \\nessa categoria. \\nPor componente \\n4 Codificadores (Encoders): s√£o modelos projetados para \\nentender (codificar) as informa√ß√µes de entrada. Eles \\ntransformam o texto em uma representa√ß√£o vetorial, \\ncapturando seu significado sem√¢ntico. Os encoders s√£o \\nfundamentais em tarefas como a compreens√£o e a', mimetype='text/plain', start_char_idx=0, end_char_idx=2887, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8212808486997736), NodeWithScore(node=TextNode(id_='6e5a57e1-8b38-4ffa-9c17-f38566cff29c', embedding=None, metadata={'page_label': '6', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3f4c349a-d20b-497c-8ffe-8a99519cd0fc', node_type='4', metadata={'page_label': '6', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='2981185eec61893f3e8ca30abe00bce41b789314d5d4fa9a536989147b4c6ba5'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8cfc1a4e-d2e7-476a-8154-43496913601c', node_type='1', metadata={'page_label': '6', 'file_name': 'LLM_2.pdf', 'file_path': '../../data/LLM_2.pdf', 'file_type': 'application/pdf', 'file_size': 427228, 'creation_date': '2025-04-26', 'last_modified_date': '2025-04-26'}, hash='04045dafb5fbd29795d87a6371ef24ed79be351decf81a05b8d2711f1e8ea0bf')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Essas \\nferramentas utilizam como insumos as conclus√µes do auditor, \\num banco de dados de relat√≥rios anteriores e um banco de \\ndados de regulamentos internos e externos aplic√°veis. A partir \\ndessas informa√ß√µes, os LLMs geram um rascunho avan√ßado do \\nrelat√≥rio de auditoria, adotando o tom, o vocabul√°rio e o estilo \\ndos auditores humanos e citando adequadamente os relat√≥rios \\nanteriores e as regulamenta√ß√µes relevantes. Isso permite que os \\nauditores economizem muito tempo em tarefas de reda√ß√£o e se \\nconcentrem em atividades de maior valor agregado. \\nEsses exemplos ilustram como os LLMs est√£o criando valor real em \\numa variedade de fun√ß√µes de neg√≥cios, desde a otimiza√ß√£o de \\nprocessos internos at√© a melhoria da experi√™ncia do cliente. Embora \\no n√∫mero de casos de uso em produ√ß√£o seja atualmente limitado, \\nespera-se que essa tend√™ncia se acelere muito rapidamente em um \\nfuturo pr√≥ximo, √† medida que os LLMs continuem a evoluir e os \\ndesafios relacionados √† privacidade e √† seguran√ßa dos dados sejam \\ntratados de forma eficaz.', mimetype='text/plain', start_char_idx=3072, end_char_idx=4105, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8195118718072266)], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.chat(\"Quais s√£o as principais ferramentas usadas em LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e755b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 8b9057b8-8345-4947-b124-4bd447c467b2. Step input: Quais as principais tendencias em LangChain que eu deveria estudar?\n"
     ]
    }
   ],
   "source": [
    "agent.chat(\"Quais as principais tendencias em LangChain que eu deveria estudar?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
