{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c8f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "521df115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = Groq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\")    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f82f2c",
   "metadata": {},
   "source": [
    "### Setting tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69c278bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker, AgentRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6088600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv # arxiv.org (archive of articles)\n",
    "\n",
    "def article_search(title: str):\n",
    "    search = arxiv.Search(\n",
    "        query=title,\n",
    "        max_results=5,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    results = [\n",
    "        f\"Title: {article.title}\\n\"\n",
    "        f\"Category: {article.primary_category}\\n\"\n",
    "        f\"Link: {article.entry_id}\\n\"\n",
    "        for article in search.results()\n",
    "    ]\n",
    "\n",
    "    return f\"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11a108a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_search_tool = FunctionTool.from_defaults(fn=article_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3dda472",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[article_search_tool],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentRunner(agent_worker)\n",
    "\n",
    "response = agent.chat(\n",
    "    \"Me retorne artigos sobre langchain na educa√ß√£o\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93191f9",
   "metadata": {},
   "source": [
    "### With Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69a74d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "\n",
    "tavily_tool = TavilyToolSpec(\n",
    "    api_key=os.environ.get(\"TAVILY_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "143f8eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n"
     ]
    }
   ],
   "source": [
    "tavily_tool_list = tavily_tool.to_tool_list()\n",
    "\n",
    "for tool in tavily_tool_list:\n",
    "    print(tool.metadata.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8246cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='d7a8999c-e23b-41d8-bbd7-e1c50a6fb00a', embedding=None, metadata={'url': 'https://medium.com/fretebras-tech/langchain-construindo-um-chatpdf-com-rag-retrieval-augmented-generation-openai-e-streamlit-61125e2122de'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Por conta desses acontecimentos, fiquei por um tempo afastado dos meus estudos. Mas por meio desse artigo compartilho meu retorno e comprometimento com os estudos, que sempre gosto de compartilhar por aqui, por meio de artigos, um pouco sobre eles.\\n\\nObrigado pela sua aten√ß√£o! E at√© breve üòÑ\\n\\n--\\n\\n--\\n\\n1\\n\\nPublished in Fretebras Tech [...] LangChain √© uma biblioteca open-source que tem como objetivo facilitar a cria√ß√£o de aplica√ß√µes utilizando LLMs, ou seja, modelos de linguagem.\\n\\nDessa forma, o LangChain disponibiliza diversos tipos de componentes e funcionalidades que facilitam desde a importa√ß√£o de diversos tipos de documentos e dados, cria√ß√£o e reutiliza√ß√£o de templates de promtps, simplificar o uso e conex√£o com uma ou v√°rias LLMs (Como ChatGPT, BERT, Llama, etc). [...] Quem nunca precisou responder a uma d√∫vida que estava em um dos muitos PDFs na pasta de Downloads, e teve que abrir cada um deles e procurar por palavras-chave?\\n\\nNeste artigo, vamos explorar a cria√ß√£o de um ChatPDF utilizando LangChain com a t√©cnica de RAG (Retrieval-Augmented Generation), OpenAI e Streamlit. O texto detalha os conceitos do LangChain e a t√©cnica de RAG, aplicando-os de maneira pr√°tica.\\n\\nLangChain', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1a9fd0d0-7f51-4c8f-a4b5-2db19df94aa1', embedding=None, metadata={'url': 'https://www.youtube.com/watch?v=seei7UaVtA4'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"This month's OpenAI meetup, held on August 27, 2024, featured a discussion led by Michael Pehel. Mike explores how AI tools like ChatGPT and LangChain can streamline content creation, particularly for generating tutorials from YouTube videos and GitHub repositories. Michael demonstrated a method that reduces manual content creation efforts by leveraging AI to handle transcription, topic generation, and code extraction, while addressing challenges like hallucinations and overconfident errors. [...] The session emphasized the importance of careful prompt engineering and selecting the right AI models for specific tasks. [...] Turn Videos into Articles with OpenAI and LangChain \\n Riis LLC \\n 7 likes \\n 121 views \\n 29 Aug 2024\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6c59d625-c8e8-41c6-97e7-6e34220b0cc5', embedding=None, metadata={'url': 'https://www.langchain.com/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[Resources Hub](/resources)[Blog](https://blog.langchain.dev/)[Customer Stories](/customers)[LangChain Academy](https://academy.langchain.com/)[Community](/community)[Experts](/experts)[Changelog](https://changelog.langchain.com/)\\n\\nDocs\\n\\nPython\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)[LangSmith](https://docs.smith.langchain.com/)[LangChain](https://python.langchain.com/docs/introduction/)\\n\\nJavaScript [...] [Resources Hub](/resources)[Blog](https://blog.langchain.dev/)[Customer Stories](/customers)[LangChain Academy](https://academy.langchain.com/)[Community](/community)[Experts](/experts)[Changelog](https://changelog.langchain.com/)\\n\\nDocs\\n\\nPython\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)[LangSmith](https://docs.smith.langchain.com/)[LangChain](https://python.langchain.com/docs/introduction/)\\n\\nJavaScript [...] ![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6723aa76cc7a8e249bd43edf_LIGHT%20BACKGROUND%20-%2031.10.2024%20-%20stack%20diagram.webp)![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667d392696fc0bc3e17a6d04_New%20LC%20stack%20-%20light-2.webp)\\n\\n20M+\\n\\nMonthly Downloads\\n\\n100K+\\n\\nApps Powered\\n\\n100K+\\n\\nGitHub Stars\\n\\n4K+\\n\\nContributors\\n\\n## The biggest developer community in GenAI\\n\\nLearn alongside the 1M+ developers who are pushing the industry forward.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_tool.search(\"Me retorne artigos cientificos sobre LangChain\", max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "370c3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_tool_function = FunctionTool.from_defaults(\n",
    "    fn=tavily_tool.search,\n",
    "    name=\"Tavily search\",\n",
    "    description=\"Busca artigos com Tavily sobre um determinado t√≥pico\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c266094",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[tavily_tool_function],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cc4a7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Me retorne artigos sobre LLM e LangChain\n",
      "=== Calling Function ===\n",
      "Calling function: Tavily search with args: {\"query\": \"LLM e LangChain\", \"max_results\": 10}\n",
      "=== Function Output ===\n",
      "[Document(id_='d6efca82-107d-47ac-95a7-511a7f467aee', embedding=None, metadata={'url': 'https://python.langchain.com/v0.1/docs/modules/model_io/llms/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='On this page\\nLLMs\\nLarge Language Models (LLMs) are a core component of LangChain. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs. To be specific, this interface is one that takes as input a string and returns a string.\\nThere are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them.\\nQuick Start\\u200b [...] Quick Start\\nLLMs\\nCustom LLM\\nCaching\\nStreaming\\nTracking token usage\\nOutput parsers\\n\\n\\n\\n\\n\\n\\n\\nRetrieval\\n\\n\\nDocument loaders\\n\\n\\nText splitters\\n\\n\\nEmbedding models\\n\\n\\nVector stores\\n\\n\\nRetrievers\\n\\n\\nIndexing\\n\\n\\nComposition\\n\\n\\nTools\\n\\n\\nAgents\\n\\n\\nChains\\n\\nMore\\n\\n\\n\\nComponents\\n\\n\\nThis is documentation for LangChain v0.1, which is no longer actively maintained.\\nFor the current stable version, see this version (Latest).\\n\\n\\nModel I/O\\nLLMs [...] LLMs | ü¶úÔ∏èüîó LangChain\\nSkip to main content\\nThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.\\nComponentsIntegrationsGuidesAPI Reference\\nMore\\n\\nPeople\\nVersioning\\nContributing\\nTemplates\\nCookbooks\\nTutorials\\nYouTube\\n\\nv0.1\\n\\nLatest\\nv0.2\\nv0.1\\n\\nü¶úÔ∏èüîó\\n\\nLangSmith\\nLangSmith Docs\\nLangServe GitHub\\nTemplates GitHub\\nTemplates Hub\\nLangChain Hub\\nJS/TS Docs\\n\\nüí¨\\nSearch\\n\\n\\nModel I/O\\n\\n\\nPrompts\\n\\n\\nChat models\\n\\n\\nLLMs', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5b4f123e-7c1b-4e6a-81fb-7f858ed075cf', embedding=None, metadata={'url': 'https://js.langchain.com/v0.1/docs/modules/chains/foundational/llm_chain/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chat\\nSearch\\nThis is documentation for LangChain v0.1, which is no longer actively maintained.\\nFor the current stable version, see this version (Latest).\\nOn this page\\nLLM\\nAn LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents. [...] An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\\nGet started\\u200b\\nWe can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM:\\ntip\\nSee this section for general instructions on installing integration packages. [...] import { OpenAI } from \"@langchain/openai\";import { LLMChain } from \"langchain/chains\";import { PromptTemplate } from \"@langchain/core/prompts\";// We can construct an LLMChain from a PromptTemplate and an LLM.const model = new OpenAI({ temperature: 0 });const prompt = PromptTemplate.fromTemplate(  \"What is a good name for a company that makes {product}?\");const chainA = new LLMChain({ llm: model, prompt });// The result is an object with a `text` property.const resA = await chainA.invoke({', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cf8d9599-2ac9-4879-bada-4c1358d5141b', embedding=None, metadata={'url': 'https://scalexi.medium.com/understanding-the-differences-between-llm-chains-and-llm-agent-executors-in-langchain-3f3cf402442f'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"LangChain has emerged as a robust framework for building applications powered by large language models (LLMs). Two fundamental concepts within LangChain are LLM Chains and LLM Agent Executors, both of which leverage tools to enhance the capabilities of LLMs. While they may seem similar at first glance, understanding their differences is crucial for developers aiming to harness LangChain's full potential. [...] Both LLM Chains and LLM Agent Executors offer powerful ways to structure and execute tasks using LangChain, but they are designed for different use cases. Understanding the key differences is essential for choosing the right approach. [...] By selecting the appropriate architecture ‚Äî whether it‚Äôs the simplicity of LLM Chains or the adaptability of LLM Agent Executors ‚Äî you can build more efficient, intelligent, and effective applications using LangChain, tailored to the specific needs of your project.\\n\\nIf you need further information or assistance, feel free to contact ScaleX Innovation at info@scalexi.ai.\\n\\n--\\n\\n--\\n\\n2\\n\\nWritten by ScaleX Innovation\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ed77cbf3-6f24-4672-92fa-a4536f1ae567', embedding=None, metadata={'url': 'https://www.stardog.com/blog/designing-llm-applications-with-knowledge-graphs-and-langchain/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LangChain is described as ‚Äúa framework for developing applications powered by language models‚Äù ‚Äî which is precisely how we use it within Voicebox. Its two central concepts for us are Chain and Vectorstore. [...] Summary\\n\\nLangChain has been a lot of fun to work with in Voicebox. It fit our needs for a framework in Python, allowing us to quickly transition from primitive, low-level code working directly with the LLM and getting every bit of mileage out of f-strings for prompt creation, to something more structured and extensible. [...] Vectorstore is pretty obvious; everyone has one in their stack. The LangChain API is well designed, and it was a trivial task for us to add support for txtai, a vector database with which we‚Äôd already had a lot of experience and tooling. This removed some learning curve hurdles, and we were able to iterate faster to more sophisticated versions of Voicebox‚Äôs core features.\\n\\nA Chain encapsulates the logic of using an LLM to accomplish a specific task. From the LangChain site:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='21aa0bf9-58ea-4542-a82c-97f9d4c251ac', embedding=None, metadata={'url': 'https://www.langchain.com/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Build\\n\\nLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.\\n\\nRun\\n\\nDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.\\n\\nManage\\n\\nLangSmith is a unified agent observability and evals platform to optimize the performance of your\\xa0AI agents - whether they're built with a LangChain framework or not. [...] ## The reference architecture enterprises adopt for success.\\n\\nLangChain‚Äôs suite of products can be used independently or stacked together for multiplicative impact ‚Äì guiding you through building, running, and managing your LLM apps. [...] [](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ce012c99a9683732d528cf_retrieval-video-transcode.mp4)\\n\\n## Build your app with LangChain\\n\\nBuild context-aware, reasoning applications with LangChain‚Äôs flexible framework that leverages your company‚Äôs data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.\\n\\n[Learn more about LangChain](/langchain)\\n\\n## Run at scale with LangGraph\\xa0Platform\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f8a021eb-f623-4c8a-976c-65f18aed2315', embedding=None, metadata={'url': 'https://www.pinecone.io/learn/series/langchain/langchain-prompt-templates/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='question cannot be answered using the information provided answer\\nwith \"I don\\'t know\".\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\nTheir superior performance over smaller models has made them incredibly\\nuseful for developers building NLP enabled applications. These models\\ncan be accessed via Hugging Face\\'s transformers library, via OpenAI\\nusing the openai library, and via Cohere using the cohere library.\\nQuestion: Which libraries and model providers offer LLMs? [...] template = \"\"\"Answer the question based on the context below. If the\\nquestion cannot be answered using the information provided answer\\nwith \"I don\\'t know\".\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\nTheir superior performance over smaller models has made them incredibly\\nuseful for developers building NLP enabled applications. These models\\ncan be accessed via Hugging Face\\'s transformers library, via OpenAI [...] prompt = \"\"\"Answer the question based on the context below. If the\\nquestion cannot be answered using the information provided answer\\nwith \"I don\\'t know\".\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\nTheir superior performance over smaller models has made them incredibly\\nuseful for developers building NLP enabled applications. These models\\ncan be accessed via Hugging Face\\'s transformers library, via OpenAI', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f232dcb0-4a78-4567-aa04-06cec59d9604', embedding=None, metadata={'url': 'https://python.langchain.com/docs/integrations/llms/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"LLMs\\ncaution\\nYou are currently on a page documenting the use of text completion models. Many of the latest and most popular models are chat completion models.\\nUnless you are specifically using more advanced prompting techniques, you are probably looking for this page instead.\\nLLMs are language models that take a string as input and return a string as output.\\ninfo\\nIf you'd like to write your own LLM, see this how-to. If you'd like to contribute an integration, see Contributing integrations. [...] | Yuan2.0 | Yuan2.0 is a new generation Fundamental Large Language Model develope... |\\nEdit this page [...] | Provider | Package |\\n| --- | --- |\\n| AI21LLM | langchain-ai21 |\\n| AnthropicLLM | langchain-anthropic |\\n| AzureOpenAI | langchain-openai |\\n| BedrockLLM | langchain-aws |\\n| CohereLLM | langchain-cohere |\\n| FireworksLLM | langchain-fireworks |\\n| OllamaLLM | langchain-ollama |\\n| OpenAILLM | langchain-openai |\\n| TogetherLLM | langchain-together |\\n| VertexAILLM | langchain-google_vertexai |\\n| NVIDIA | langchain-nvidia |\\nAll LLMs\\u200b\\n| Name | Description |\\n| --- | --- |\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='616cd9a6-0236-4ac1-b856-11965e3bdeda', embedding=None, metadata={'url': 'https://www.youtube.com/watch?v=AjQPRomyd-k'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"LLM Project | End to End Gen AI Project Using LangChain, Google Palm In Ed-Tech Industry \\n codebasics \\n 2910 likes \\n 119522 views \\n 27 Oct 2023 \\n This is an End-to-End LLM project using the langchain framework. We are building a question-and-answer system for a real e-learning company (no toy datasets). If you are new to GenAI and LLM application development, then this langchain tutorial will help you understand how to build the end-to-end application. [...] we are going to build end to endend llm project where we will use Lang chain hugging face streamlet and Google Palm model the use case we are solving is for a real e-learning company we have not used any toy data set here that company is my own company code Basics we have this code basic. platform which has many data related courses with more than 20,000 students now these existing students or a person who wants to buy the course they will have questions related to courses fees Etc and we will [...] this requirement. txt file see this txt file has all these modules which will be using in our project and you can go to this folder and simply run p install r hyph r requirement. txt and it will install all the modules by the way I'm giving all this code check in video description you'll be able to download all this code okay it has this faq.com text file and from this place now I'm going to import Google prom so you'll say from Lang chain. llm import Google pal and then llm object is equal to\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b304a8b2-590f-48d4-8950-dfbbb0e41b20', embedding=None, metadata={'url': 'https://www.reddit.com/r/LangChain/comments/1al25wz/langchain_for_llm/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='You can use pretty much most of the new fully local models. Langchain supports Ollama through their open ai API support so you can use RAG using', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0f98c5e7-43a5-4c5c-9898-a63a5fe8eae6', embedding=None, metadata={'url': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Whether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to the global verbose value,\\naccessible via langchain.globals.get_verbose().\\n\\nCreate LLMChain from LLM and template.\\n\\nllm (BaseLanguageModel)\\n\\ntemplate (str)\\n\\nLLMChain\\n\\nDeprecated since version 0.1.0: Use invoke() instead. It will not be removed until langchain==1.0.\\n\\nExecute the chain. [...] Section Navigation\\n\\nBase packages\\n\\nIntegrations\\n\\nLLMChain#\\n\\nBases: Chain\\n\\nDeprecated since version 0.1.17: Use , `prompt | llm`() instead. It will not be removed until langchain==1.0.\\n\\nChain to run queries against LLMs.\\n\\nThis class is deprecated. See below for an example implementation using\\nLangChain runnables:\\n\\nExample\\n\\nNote\\n\\nLLMChain implements the standard Runnable Interface. üèÉ [...] Call apply and then parse the results.\\n\\ninput_list (List[Dict[str, Any]])\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\nSequence[str | List[str] | Dict[str, str]]\\n\\nCall apredict and then parse the results.\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\nkwargs (Any)\\n\\nstr | List[str] | Dict[str, str]\\n\\nPrepare chain inputs, including adding inputs from memory.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n",
      "=== LLM Response ===\n",
      "Aqui est√£o alguns artigos sobre LLM e LangChain:\n",
      "\n",
      "1. \"LLMs\" da documenta√ß√£o do LangChain: https://python.langchain.com/v0.1/docs/modules/model_io/llms/\n",
      "2. \"LLM Chain\" da documenta√ß√£o do LangChain: https://js.langchain.com/v0.1/docs/modules/chains/foundational/llm_chain/\n",
      "3. \"Designing LLM Applications with Knowledge Graphs and LangChain\" do blog da Stardog: https://www.stardog.com/blog/designing-llm-applications-with-knowledge-graphs-and-langchain/\n",
      "4. \"LangChain\" do site oficial do LangChain: https://www.langchain.com/\n",
      "5. \"LLM Project | End to End Gen AI Project Using LangChain, Google Palm In Ed-Tech Industry\" do YouTube: https://www.youtube.com/watch?v=AjQPRomyd-k\n",
      "6. \"LangChain for LLM\" do Reddit: https://www.reddit.com/r/LangChain/comments/1al25wz/langchain_for_llm/\n",
      "7. \"LLMChain\" da documenta√ß√£o do LangChain: https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html\n",
      "\n",
      "Esses artigos fornecem informa√ß√µes sobre como usar o LangChain para desenvolver aplica√ß√µes com LLMs, incluindo exemplos de c√≥digo e tutoriais. Al√©m disso, eles discutem as vantagens e desvantagens de usar o LangChain e como ele pode ser utilizado em diferentes contextos.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='Aqui est√£o alguns artigos sobre LLM e LangChain:\\n\\n1. \"LLMs\" da documenta√ß√£o do LangChain: https://python.langchain.com/v0.1/docs/modules/model_io/llms/\\n2. \"LLM Chain\" da documenta√ß√£o do LangChain: https://js.langchain.com/v0.1/docs/modules/chains/foundational/llm_chain/\\n3. \"Designing LLM Applications with Knowledge Graphs and LangChain\" do blog da Stardog: https://www.stardog.com/blog/designing-llm-applications-with-knowledge-graphs-and-langchain/\\n4. \"LangChain\" do site oficial do LangChain: https://www.langchain.com/\\n5. \"LLM Project | End to End Gen AI Project Using LangChain, Google Palm In Ed-Tech Industry\" do YouTube: https://www.youtube.com/watch?v=AjQPRomyd-k\\n6. \"LangChain for LLM\" do Reddit: https://www.reddit.com/r/LangChain/comments/1al25wz/langchain_for_llm/\\n7. \"LLMChain\" da documenta√ß√£o do LangChain: https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html\\n\\nEsses artigos fornecem informa√ß√µes sobre como usar o LangChain para desenvolver aplica√ß√µes com LLMs, incluindo exemplos de c√≥digo e tutoriais. Al√©m disso, eles discutem as vantagens e desvantagens de usar o LangChain e como ele pode ser utilizado em diferentes contextos.', sources=[ToolOutput(content='[Document(id_=\\'d6efca82-107d-47ac-95a7-511a7f467aee\\', embedding=None, metadata={\\'url\\': \\'https://python.langchain.com/v0.1/docs/modules/model_io/llms/\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\\'On this page\\\\nLLMs\\\\nLarge Language Models (LLMs) are a core component of LangChain. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs. To be specific, this interface is one that takes as input a string and returns a string.\\\\nThere are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them.\\\\nQuick Start\\\\u200b [...] Quick Start\\\\nLLMs\\\\nCustom LLM\\\\nCaching\\\\nStreaming\\\\nTracking token usage\\\\nOutput parsers\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nRetrieval\\\\n\\\\n\\\\nDocument loaders\\\\n\\\\n\\\\nText splitters\\\\n\\\\n\\\\nEmbedding models\\\\n\\\\n\\\\nVector stores\\\\n\\\\n\\\\nRetrievers\\\\n\\\\n\\\\nIndexing\\\\n\\\\n\\\\nComposition\\\\n\\\\n\\\\nTools\\\\n\\\\n\\\\nAgents\\\\n\\\\n\\\\nChains\\\\n\\\\nMore\\\\n\\\\n\\\\n\\\\nComponents\\\\n\\\\n\\\\nThis is documentation for LangChain v0.1, which is no longer actively maintained.\\\\nFor the current stable version, see this version (Latest).\\\\n\\\\n\\\\nModel I/O\\\\nLLMs [...] LLMs | ü¶úÔ∏èüîó LangChain\\\\nSkip to main content\\\\nThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.\\\\nComponentsIntegrationsGuidesAPI Reference\\\\nMore\\\\n\\\\nPeople\\\\nVersioning\\\\nContributing\\\\nTemplates\\\\nCookbooks\\\\nTutorials\\\\nYouTube\\\\n\\\\nv0.1\\\\n\\\\nLatest\\\\nv0.2\\\\nv0.1\\\\n\\\\nü¶úÔ∏èüîó\\\\n\\\\nLangSmith\\\\nLangSmith Docs\\\\nLangServe GitHub\\\\nTemplates GitHub\\\\nTemplates Hub\\\\nLangChain Hub\\\\nJS/TS Docs\\\\n\\\\nüí¨\\\\nSearch\\\\n\\\\n\\\\nModel I/O\\\\n\\\\n\\\\nPrompts\\\\n\\\\n\\\\nChat models\\\\n\\\\n\\\\nLLMs\\', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'5b4f123e-7c1b-4e6a-81fb-7f858ed075cf\\', embedding=None, metadata={\\'url\\': \\'https://js.langchain.com/v0.1/docs/modules/chains/foundational/llm_chain/\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\\'Chat\\\\nSearch\\\\nThis is documentation for LangChain v0.1, which is no longer actively maintained.\\\\nFor the current stable version, see this version (Latest).\\\\nOn this page\\\\nLLM\\\\nAn LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents. [...] An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\\\\nGet started\\\\u200b\\\\nWe can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM:\\\\ntip\\\\nSee this section for general instructions on installing integration packages. [...] import { OpenAI } from \"@langchain/openai\";import { LLMChain } from \"langchain/chains\";import { PromptTemplate } from \"@langchain/core/prompts\";// We can construct an LLMChain from a PromptTemplate and an LLM.const model = new OpenAI({ temperature: 0 });const prompt = PromptTemplate.fromTemplate(  \"What is a good name for a company that makes {product}?\");const chainA = new LLMChain({ llm: model, prompt });// The result is an object with a `text` property.const resA = await chainA.invoke({\\', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'cf8d9599-2ac9-4879-bada-4c1358d5141b\\', embedding=None, metadata={\\'url\\': \\'https://scalexi.medium.com/understanding-the-differences-between-llm-chains-and-llm-agent-executors-in-langchain-3f3cf402442f\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\"LangChain has emerged as a robust framework for building applications powered by large language models (LLMs). Two fundamental concepts within LangChain are LLM Chains and LLM Agent Executors, both of which leverage tools to enhance the capabilities of LLMs. While they may seem similar at first glance, understanding their differences is crucial for developers aiming to harness LangChain\\'s full potential. [...] Both LLM Chains and LLM Agent Executors offer powerful ways to structure and execute tasks using LangChain, but they are designed for different use cases. Understanding the key differences is essential for choosing the right approach. [...] By selecting the appropriate architecture ‚Äî whether it‚Äôs the simplicity of LLM Chains or the adaptability of LLM Agent Executors ‚Äî you can build more efficient, intelligent, and effective applications using LangChain, tailored to the specific needs of your project.\\\\n\\\\nIf you need further information or assistance, feel free to contact ScaleX Innovation at info@scalexi.ai.\\\\n\\\\n--\\\\n\\\\n--\\\\n\\\\n2\\\\n\\\\nWritten by ScaleX Innovation\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'ed77cbf3-6f24-4672-92fa-a4536f1ae567\\', embedding=None, metadata={\\'url\\': \\'https://www.stardog.com/blog/designing-llm-applications-with-knowledge-graphs-and-langchain/\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\\'LangChain is described as ‚Äúa framework for developing applications powered by language models‚Äù ‚Äî which is precisely how we use it within Voicebox. Its two central concepts for us are Chain and Vectorstore. [...] Summary\\\\n\\\\nLangChain has been a lot of fun to work with in Voicebox. It fit our needs for a framework in Python, allowing us to quickly transition from primitive, low-level code working directly with the LLM and getting every bit of mileage out of f-strings for prompt creation, to something more structured and extensible. [...] Vectorstore is pretty obvious; everyone has one in their stack. The LangChain API is well designed, and it was a trivial task for us to add support for txtai, a vector database with which we‚Äôd already had a lot of experience and tooling. This removed some learning curve hurdles, and we were able to iterate faster to more sophisticated versions of Voicebox‚Äôs core features.\\\\n\\\\nA Chain encapsulates the logic of using an LLM to accomplish a specific task. From the LangChain site:\\', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'21aa0bf9-58ea-4542-a82c-97f9d4c251ac\\', embedding=None, metadata={\\'url\\': \\'https://www.langchain.com/\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\"Build\\\\n\\\\nLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.\\\\n\\\\nRun\\\\n\\\\nDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.\\\\n\\\\nManage\\\\n\\\\nLangSmith is a unified agent observability and evals platform to optimize the performance of your\\\\xa0AI agents - whether they\\'re built with a LangChain framework or not. [...] ## The reference architecture enterprises adopt for success.\\\\n\\\\nLangChain‚Äôs suite of products can be used independently or stacked together for multiplicative impact ‚Äì guiding you through building, running, and managing your LLM apps. [...] [](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ce012c99a9683732d528cf_retrieval-video-transcode.mp4)\\\\n\\\\n## Build your app with LangChain\\\\n\\\\nBuild context-aware, reasoning applications with LangChain‚Äôs flexible framework that leverages your company‚Äôs data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.\\\\n\\\\n[Learn more about LangChain](/langchain)\\\\n\\\\n## Run at scale with LangGraph\\\\xa0Platform\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'f8a021eb-f623-4c8a-976c-65f18aed2315\\', embedding=None, metadata={\\'url\\': \\'https://www.pinecone.io/learn/series/langchain/langchain-prompt-templates/\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\\'question cannot be answered using the information provided answer\\\\nwith \"I don\\\\\\'t know\".\\\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\\\nTheir superior performance over smaller models has made them incredibly\\\\nuseful for developers building NLP enabled applications. These models\\\\ncan be accessed via Hugging Face\\\\\\'s transformers library, via OpenAI\\\\nusing the openai library, and via Cohere using the cohere library.\\\\nQuestion: Which libraries and model providers offer LLMs? [...] template = \"\"\"Answer the question based on the context below. If the\\\\nquestion cannot be answered using the information provided answer\\\\nwith \"I don\\\\\\'t know\".\\\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\\\nTheir superior performance over smaller models has made them incredibly\\\\nuseful for developers building NLP enabled applications. These models\\\\ncan be accessed via Hugging Face\\\\\\'s transformers library, via OpenAI [...] prompt = \"\"\"Answer the question based on the context below. If the\\\\nquestion cannot be answered using the information provided answer\\\\nwith \"I don\\\\\\'t know\".\\\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\\\nTheir superior performance over smaller models has made them incredibly\\\\nuseful for developers building NLP enabled applications. These models\\\\ncan be accessed via Hugging Face\\\\\\'s transformers library, via OpenAI\\', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'f232dcb0-4a78-4567-aa04-06cec59d9604\\', embedding=None, metadata={\\'url\\': \\'https://python.langchain.com/docs/integrations/llms/\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\"LLMs\\\\ncaution\\\\nYou are currently on a page documenting the use of text completion models. Many of the latest and most popular models are chat completion models.\\\\nUnless you are specifically using more advanced prompting techniques, you are probably looking for this page instead.\\\\nLLMs are language models that take a string as input and return a string as output.\\\\ninfo\\\\nIf you\\'d like to write your own LLM, see this how-to. If you\\'d like to contribute an integration, see Contributing integrations. [...] | Yuan2.0 | Yuan2.0 is a new generation Fundamental Large Language Model develope... |\\\\nEdit this page [...] | Provider | Package |\\\\n| --- | --- |\\\\n| AI21LLM | langchain-ai21 |\\\\n| AnthropicLLM | langchain-anthropic |\\\\n| AzureOpenAI | langchain-openai |\\\\n| BedrockLLM | langchain-aws |\\\\n| CohereLLM | langchain-cohere |\\\\n| FireworksLLM | langchain-fireworks |\\\\n| OllamaLLM | langchain-ollama |\\\\n| OpenAILLM | langchain-openai |\\\\n| TogetherLLM | langchain-together |\\\\n| VertexAILLM | langchain-google_vertexai |\\\\n| NVIDIA | langchain-nvidia |\\\\nAll LLMs\\\\u200b\\\\n| Name | Description |\\\\n| --- | --- |\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'616cd9a6-0236-4ac1-b856-11965e3bdeda\\', embedding=None, metadata={\\'url\\': \\'https://www.youtube.com/watch?v=AjQPRomyd-k\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\"LLM Project | End to End Gen AI Project Using LangChain, Google Palm In Ed-Tech Industry \\\\n codebasics \\\\n 2910 likes \\\\n 119522 views \\\\n 27 Oct 2023 \\\\n This is an End-to-End LLM project using the langchain framework. We are building a question-and-answer system for a real e-learning company (no toy datasets). If you are new to GenAI and LLM application development, then this langchain tutorial will help you understand how to build the end-to-end application. [...] we are going to build end to endend llm project where we will use Lang chain hugging face streamlet and Google Palm model the use case we are solving is for a real e-learning company we have not used any toy data set here that company is my own company code Basics we have this code basic. platform which has many data related courses with more than 20,000 students now these existing students or a person who wants to buy the course they will have questions related to courses fees Etc and we will [...] this requirement. txt file see this txt file has all these modules which will be using in our project and you can go to this folder and simply run p install r hyph r requirement. txt and it will install all the modules by the way I\\'m giving all this code check in video description you\\'ll be able to download all this code okay it has this faq.com text file and from this place now I\\'m going to import Google prom so you\\'ll say from Lang chain. llm import Google pal and then llm object is equal to\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'b304a8b2-590f-48d4-8950-dfbbb0e41b20\\', embedding=None, metadata={\\'url\\': \\'https://www.reddit.com/r/LangChain/comments/1al25wz/langchain_for_llm/\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\\'You can use pretty much most of the new fully local models. Langchain supports Ollama through their open ai API support so you can use RAG using\\', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), Document(id_=\\'0f98c5e7-43a5-4c5c-9898-a63a5fe8eae6\\', embedding=None, metadata={\\'url\\': \\'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html\\'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text_resource=MediaResource(embeddings=None, data=None, text=\\'Whether or not run in verbose mode. In verbose mode, some intermediate logs\\\\nwill be printed to the console. Defaults to the global verbose value,\\\\naccessible via langchain.globals.get_verbose().\\\\n\\\\nCreate LLMChain from LLM and template.\\\\n\\\\nllm (BaseLanguageModel)\\\\n\\\\ntemplate (str)\\\\n\\\\nLLMChain\\\\n\\\\nDeprecated since version 0.1.0: Use invoke() instead. It will not be removed until langchain==1.0.\\\\n\\\\nExecute the chain. [...] Section Navigation\\\\n\\\\nBase packages\\\\n\\\\nIntegrations\\\\n\\\\nLLMChain#\\\\n\\\\nBases: Chain\\\\n\\\\nDeprecated since version 0.1.17: Use , `prompt | llm`() instead. It will not be removed until langchain==1.0.\\\\n\\\\nChain to run queries against LLMs.\\\\n\\\\nThis class is deprecated. See below for an example implementation using\\\\nLangChain runnables:\\\\n\\\\nExample\\\\n\\\\nNote\\\\n\\\\nLLMChain implements the standard Runnable Interface. üèÉ [...] Call apply and then parse the results.\\\\n\\\\ninput_list (List[Dict[str, Any]])\\\\n\\\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\\\n\\\\nSequence[str | List[str] | Dict[str, str]]\\\\n\\\\nCall apredict and then parse the results.\\\\n\\\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\\\n\\\\nkwargs (Any)\\\\n\\\\nstr | List[str] | Dict[str, str]\\\\n\\\\nPrepare chain inputs, including adding inputs from memory.\\', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\')]', tool_name='Tavily search', raw_input={'args': (), 'kwargs': {'query': 'LLM e LangChain', 'max_results': 10}}, raw_output=[Document(id_='d6efca82-107d-47ac-95a7-511a7f467aee', embedding=None, metadata={'url': 'https://python.langchain.com/v0.1/docs/modules/model_io/llms/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='On this page\\nLLMs\\nLarge Language Models (LLMs) are a core component of LangChain. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs. To be specific, this interface is one that takes as input a string and returns a string.\\nThere are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them.\\nQuick Start\\u200b [...] Quick Start\\nLLMs\\nCustom LLM\\nCaching\\nStreaming\\nTracking token usage\\nOutput parsers\\n\\n\\n\\n\\n\\n\\n\\nRetrieval\\n\\n\\nDocument loaders\\n\\n\\nText splitters\\n\\n\\nEmbedding models\\n\\n\\nVector stores\\n\\n\\nRetrievers\\n\\n\\nIndexing\\n\\n\\nComposition\\n\\n\\nTools\\n\\n\\nAgents\\n\\n\\nChains\\n\\nMore\\n\\n\\n\\nComponents\\n\\n\\nThis is documentation for LangChain v0.1, which is no longer actively maintained.\\nFor the current stable version, see this version (Latest).\\n\\n\\nModel I/O\\nLLMs [...] LLMs | ü¶úÔ∏èüîó LangChain\\nSkip to main content\\nThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.\\nComponentsIntegrationsGuidesAPI Reference\\nMore\\n\\nPeople\\nVersioning\\nContributing\\nTemplates\\nCookbooks\\nTutorials\\nYouTube\\n\\nv0.1\\n\\nLatest\\nv0.2\\nv0.1\\n\\nü¶úÔ∏èüîó\\n\\nLangSmith\\nLangSmith Docs\\nLangServe GitHub\\nTemplates GitHub\\nTemplates Hub\\nLangChain Hub\\nJS/TS Docs\\n\\nüí¨\\nSearch\\n\\n\\nModel I/O\\n\\n\\nPrompts\\n\\n\\nChat models\\n\\n\\nLLMs', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5b4f123e-7c1b-4e6a-81fb-7f858ed075cf', embedding=None, metadata={'url': 'https://js.langchain.com/v0.1/docs/modules/chains/foundational/llm_chain/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Chat\\nSearch\\nThis is documentation for LangChain v0.1, which is no longer actively maintained.\\nFor the current stable version, see this version (Latest).\\nOn this page\\nLLM\\nAn LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents. [...] An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\\nGet started\\u200b\\nWe can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM:\\ntip\\nSee this section for general instructions on installing integration packages. [...] import { OpenAI } from \"@langchain/openai\";import { LLMChain } from \"langchain/chains\";import { PromptTemplate } from \"@langchain/core/prompts\";// We can construct an LLMChain from a PromptTemplate and an LLM.const model = new OpenAI({ temperature: 0 });const prompt = PromptTemplate.fromTemplate(  \"What is a good name for a company that makes {product}?\");const chainA = new LLMChain({ llm: model, prompt });// The result is an object with a `text` property.const resA = await chainA.invoke({', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cf8d9599-2ac9-4879-bada-4c1358d5141b', embedding=None, metadata={'url': 'https://scalexi.medium.com/understanding-the-differences-between-llm-chains-and-llm-agent-executors-in-langchain-3f3cf402442f'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"LangChain has emerged as a robust framework for building applications powered by large language models (LLMs). Two fundamental concepts within LangChain are LLM Chains and LLM Agent Executors, both of which leverage tools to enhance the capabilities of LLMs. While they may seem similar at first glance, understanding their differences is crucial for developers aiming to harness LangChain's full potential. [...] Both LLM Chains and LLM Agent Executors offer powerful ways to structure and execute tasks using LangChain, but they are designed for different use cases. Understanding the key differences is essential for choosing the right approach. [...] By selecting the appropriate architecture ‚Äî whether it‚Äôs the simplicity of LLM Chains or the adaptability of LLM Agent Executors ‚Äî you can build more efficient, intelligent, and effective applications using LangChain, tailored to the specific needs of your project.\\n\\nIf you need further information or assistance, feel free to contact ScaleX Innovation at info@scalexi.ai.\\n\\n--\\n\\n--\\n\\n2\\n\\nWritten by ScaleX Innovation\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ed77cbf3-6f24-4672-92fa-a4536f1ae567', embedding=None, metadata={'url': 'https://www.stardog.com/blog/designing-llm-applications-with-knowledge-graphs-and-langchain/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LangChain is described as ‚Äúa framework for developing applications powered by language models‚Äù ‚Äî which is precisely how we use it within Voicebox. Its two central concepts for us are Chain and Vectorstore. [...] Summary\\n\\nLangChain has been a lot of fun to work with in Voicebox. It fit our needs for a framework in Python, allowing us to quickly transition from primitive, low-level code working directly with the LLM and getting every bit of mileage out of f-strings for prompt creation, to something more structured and extensible. [...] Vectorstore is pretty obvious; everyone has one in their stack. The LangChain API is well designed, and it was a trivial task for us to add support for txtai, a vector database with which we‚Äôd already had a lot of experience and tooling. This removed some learning curve hurdles, and we were able to iterate faster to more sophisticated versions of Voicebox‚Äôs core features.\\n\\nA Chain encapsulates the logic of using an LLM to accomplish a specific task. From the LangChain site:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='21aa0bf9-58ea-4542-a82c-97f9d4c251ac', embedding=None, metadata={'url': 'https://www.langchain.com/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Build\\n\\nLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.\\n\\nRun\\n\\nDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.\\n\\nManage\\n\\nLangSmith is a unified agent observability and evals platform to optimize the performance of your\\xa0AI agents - whether they're built with a LangChain framework or not. [...] ## The reference architecture enterprises adopt for success.\\n\\nLangChain‚Äôs suite of products can be used independently or stacked together for multiplicative impact ‚Äì guiding you through building, running, and managing your LLM apps. [...] [](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ce012c99a9683732d528cf_retrieval-video-transcode.mp4)\\n\\n## Build your app with LangChain\\n\\nBuild context-aware, reasoning applications with LangChain‚Äôs flexible framework that leverages your company‚Äôs data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.\\n\\n[Learn more about LangChain](/langchain)\\n\\n## Run at scale with LangGraph\\xa0Platform\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f8a021eb-f623-4c8a-976c-65f18aed2315', embedding=None, metadata={'url': 'https://www.pinecone.io/learn/series/langchain/langchain-prompt-templates/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='question cannot be answered using the information provided answer\\nwith \"I don\\'t know\".\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\nTheir superior performance over smaller models has made them incredibly\\nuseful for developers building NLP enabled applications. These models\\ncan be accessed via Hugging Face\\'s transformers library, via OpenAI\\nusing the openai library, and via Cohere using the cohere library.\\nQuestion: Which libraries and model providers offer LLMs? [...] template = \"\"\"Answer the question based on the context below. If the\\nquestion cannot be answered using the information provided answer\\nwith \"I don\\'t know\".\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\nTheir superior performance over smaller models has made them incredibly\\nuseful for developers building NLP enabled applications. These models\\ncan be accessed via Hugging Face\\'s transformers library, via OpenAI [...] prompt = \"\"\"Answer the question based on the context below. If the\\nquestion cannot be answered using the information provided answer\\nwith \"I don\\'t know\".\\nContext: Large Language Models (LLMs) are the latest models used in NLP.\\nTheir superior performance over smaller models has made them incredibly\\nuseful for developers building NLP enabled applications. These models\\ncan be accessed via Hugging Face\\'s transformers library, via OpenAI', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f232dcb0-4a78-4567-aa04-06cec59d9604', embedding=None, metadata={'url': 'https://python.langchain.com/docs/integrations/llms/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"LLMs\\ncaution\\nYou are currently on a page documenting the use of text completion models. Many of the latest and most popular models are chat completion models.\\nUnless you are specifically using more advanced prompting techniques, you are probably looking for this page instead.\\nLLMs are language models that take a string as input and return a string as output.\\ninfo\\nIf you'd like to write your own LLM, see this how-to. If you'd like to contribute an integration, see Contributing integrations. [...] | Yuan2.0 | Yuan2.0 is a new generation Fundamental Large Language Model develope... |\\nEdit this page [...] | Provider | Package |\\n| --- | --- |\\n| AI21LLM | langchain-ai21 |\\n| AnthropicLLM | langchain-anthropic |\\n| AzureOpenAI | langchain-openai |\\n| BedrockLLM | langchain-aws |\\n| CohereLLM | langchain-cohere |\\n| FireworksLLM | langchain-fireworks |\\n| OllamaLLM | langchain-ollama |\\n| OpenAILLM | langchain-openai |\\n| TogetherLLM | langchain-together |\\n| VertexAILLM | langchain-google_vertexai |\\n| NVIDIA | langchain-nvidia |\\nAll LLMs\\u200b\\n| Name | Description |\\n| --- | --- |\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='616cd9a6-0236-4ac1-b856-11965e3bdeda', embedding=None, metadata={'url': 'https://www.youtube.com/watch?v=AjQPRomyd-k'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"LLM Project | End to End Gen AI Project Using LangChain, Google Palm In Ed-Tech Industry \\n codebasics \\n 2910 likes \\n 119522 views \\n 27 Oct 2023 \\n This is an End-to-End LLM project using the langchain framework. We are building a question-and-answer system for a real e-learning company (no toy datasets). If you are new to GenAI and LLM application development, then this langchain tutorial will help you understand how to build the end-to-end application. [...] we are going to build end to endend llm project where we will use Lang chain hugging face streamlet and Google Palm model the use case we are solving is for a real e-learning company we have not used any toy data set here that company is my own company code Basics we have this code basic. platform which has many data related courses with more than 20,000 students now these existing students or a person who wants to buy the course they will have questions related to courses fees Etc and we will [...] this requirement. txt file see this txt file has all these modules which will be using in our project and you can go to this folder and simply run p install r hyph r requirement. txt and it will install all the modules by the way I'm giving all this code check in video description you'll be able to download all this code okay it has this faq.com text file and from this place now I'm going to import Google prom so you'll say from Lang chain. llm import Google pal and then llm object is equal to\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b304a8b2-590f-48d4-8950-dfbbb0e41b20', embedding=None, metadata={'url': 'https://www.reddit.com/r/LangChain/comments/1al25wz/langchain_for_llm/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='You can use pretty much most of the new fully local models. Langchain supports Ollama through their open ai API support so you can use RAG using', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0f98c5e7-43a5-4c5c-9898-a63a5fe8eae6', embedding=None, metadata={'url': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Whether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to the global verbose value,\\naccessible via langchain.globals.get_verbose().\\n\\nCreate LLMChain from LLM and template.\\n\\nllm (BaseLanguageModel)\\n\\ntemplate (str)\\n\\nLLMChain\\n\\nDeprecated since version 0.1.0: Use invoke() instead. It will not be removed until langchain==1.0.\\n\\nExecute the chain. [...] Section Navigation\\n\\nBase packages\\n\\nIntegrations\\n\\nLLMChain#\\n\\nBases: Chain\\n\\nDeprecated since version 0.1.17: Use , `prompt | llm`() instead. It will not be removed until langchain==1.0.\\n\\nChain to run queries against LLMs.\\n\\nThis class is deprecated. See below for an example implementation using\\nLangChain runnables:\\n\\nExample\\n\\nNote\\n\\nLLMChain implements the standard Runnable Interface. üèÉ [...] Call apply and then parse the results.\\n\\ninput_list (List[Dict[str, Any]])\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\nSequence[str | List[str] | Dict[str, str]]\\n\\nCall apredict and then parse the results.\\n\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)\\n\\nkwargs (Any)\\n\\nstr | List[str] | Dict[str, str]\\n\\nPrepare chain inputs, including adding inputs from memory.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')], is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = AgentRunner(agent_worker)\n",
    "\n",
    "agent.chat(\"Me retorne artigos sobre LLM e LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ab90ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "617de287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 108 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "url = \"../../data/LLM.pdf\"\n",
    "article = SimpleDirectoryReader(input_files=[url]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31d8edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../../data/LLM_2.pdf\"\n",
    "tutorial = SimpleDirectoryReader(input_files=[url]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35d2c4",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4da80298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bc86032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817ba6f2317c40dba05df6ec9b60c8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a0fb4af579421db96e1293bdfa5289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/160k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a469858f968c45c2b121d1de75132b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852fef67e41b4b849b557e012ccbc7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19371787f2dd4a61aae3fecb32c90126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73fc3028d404952913dd131851b34ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d706310b376b43ca9ccf5527c09f720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e7a3ab087b46219559e63e09f3f35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b80ccf762544598338c4ceefde7ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f65c57ac26e415f993bc59fb5f64801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"intfloat/multilingual-e5-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96d95533",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_index = VectorStoreIndex.from_documents(article)\n",
    "tutorial_index = VectorStoreIndex.from_documents(tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbaf1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_index.storage_context.persist(persist_dir=\"embedding/article\")\n",
    "tutorial_index.storage_context.persist(persist_dir=\"embedding/tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb95bd",
   "metadata": {},
   "source": [
    "### Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6288d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.tools import QueryEngineTool,ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fad8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"embedding/article\"\n",
    ")\n",
    "\n",
    "article_index = load_index_from_storage(article_storage_context)\n",
    "\n",
    "tutorial_storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"embedding/tutorial\"\n",
    ")\n",
    "\n",
    "tutorial_index = load_index_from_storage(tutorial_storage_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ce3f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_engine = article_index.as_query_engine(similarity_top_k=3, llm=llm)\n",
    "tutorial_engine = tutorial_index.as_query_engine(similarity_top_k=3, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "129ea0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=article_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"article_engine\",\n",
    "            description=(\n",
    "                \"Fornece informa√ß√µes sobre LLM e LangChain\"\n",
    "                \"Use uma pergunta detalhada em texto simples como entrada para a ferramenta\"\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=tutorial_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"tutorial_engine\",\n",
    "            description=(\n",
    "                \"Fornece informa√ß√µes sobre casos de uso e aplica√ß√µes em LLMs\"\n",
    "                \"Use uma pergunta detalhada em texto simples como entrada para a ferramenta\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7eee6ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    query_engine_tools,\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=True,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "agent_document = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67760f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Quais as principais aplica√ß√µes que posso construir com LLM e LangChain?\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es que posso construir com LLM e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplica√ß√µes que voc√™ pode construir com LLM (Large Language Models) e LangChain incluem:\n",
      "\n",
      "1. **Cria√ß√£o e aprimoramento de conte√∫do**: Gera√ß√£o autom√°tica de texto, assist√™ncia na reda√ß√£o, tradu√ß√£o autom√°tica, resumo de textos, planejamento e roteiro de conte√∫do, brainstorming e programa√ß√£o.\n",
      "2. **An√°lise e organiza√ß√£o de informa√ß√µes**: An√°lise de sentimento, extra√ß√£o de informa√ß√µes, classifica√ß√£o de textos, revis√£o t√©cnica e outras aplica√ß√µes de processamento de linguagem natural.\n",
      "3. **Intera√ß√£o e automa√ß√£o**: Chatbots, perguntas e respostas, gera√ß√£o de respostas a perguntas com base em um corpus, e outras aplica√ß√µes de intera√ß√£o com usu√°rios.\n",
      "4. **Desenvolvimento de software**: Uso de LLMs para auxiliar programadores, acelerar a gera√ß√£o de c√≥digo e melhorar a qualidade do desenvolvimento de software, como no caso do GitHub Copilot ou do StarCoder.\n",
      "5. **Ferramentas de escrit√≥rio**: Integra√ß√£o de LLMs √†s ferramentas de desenvolvimento de software e de escrit√≥rio, como o Microsoft 365 Copilot e o Google Workspace, para transformar a efici√™ncia e a capacidade das empresas.\n",
      "\n",
      "Al√©m disso, com o LangChain, voc√™ pode construir aplica√ß√µes que combinam LLMs com outras tecnologias, como blockchain, para criar solu√ß√µes inovadoras e seguras para diversas ind√∫strias.\n",
      "=== Calling Function ===\n",
      "Calling function: article_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es que posso construir com LLM e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplica√ß√µes que voc√™ pode construir com LLMs incluem: \n",
      "\n",
      "1. Chatbots e assistentes virtuais para fornecer suporte ao cliente, solu√ß√£o de problemas ou conversas abertas.\n",
      "2. Gera√ß√£o de c√≥digo e depura√ß√£o para fornecer trechos √∫teis de c√≥digo como resposta a solicita√ß√µes escritas em linguagem natural.\n",
      "3. An√°lise de sentimento para ajudar a analisar emo√ß√µes e opini√µes a partir de um texto.\n",
      "4. Classifica√ß√£o e agrupamento de texto para categorizar e classificar grandes volumes de dados.\n",
      "5. Tradu√ß√£o de idiomas para globalizar todo o seu conte√∫do sem horas de trabalho √°rduo.\n",
      "6. Resumo e par√°fraseamento para resumir chamadas ou reuni√µes de clientes de forma eficiente.\n",
      "7. Gera√ß√£o de conte√∫do para desenvolver um esbo√ßo ou gerar um primeiro rascunho a partir de um prompt detalhado.\n",
      "\n",
      "Essas s√£o apenas algumas das muitas aplica√ß√µes poss√≠veis com LLMs, e a escolha da aplica√ß√£o certa depender√° das necessidades espec√≠ficas do seu projeto ou organiza√ß√£o.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es que posso construir com LLM e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplica√ß√µes que voc√™ pode construir com LLM (Large Language Models) e LangChain incluem:\n",
      "\n",
      "1. **Cria√ß√£o e aprimoramento de conte√∫do**: Voc√™ pode usar LLMs para gerar conte√∫do automaticamente, como textos, resumos, tradu√ß√µes e at√© mesmo c√≥digos de programa√ß√£o.\n",
      "2. **An√°lise e organiza√ß√£o de informa√ß√µes**: LLMs podem ser usados para analisar sentimentos, extrair informa√ß√µes espec√≠ficas de documentos grandes, classificar textos em categorias ou temas espec√≠ficos e revisar documentos especializados.\n",
      "3. **Intera√ß√£o e automa√ß√£o**: Voc√™ pode construir chatbots que simulam conversas sobre t√≥picos gerais ou espec√≠ficos, gerar respostas a perguntas com base em um corpus e criar experi√™ncias interativas ricas com entrada n√£o apenas de texto, mas tamb√©m de imagem, √°udio e v√≠deo.\n",
      "4. **Desenvolvimento de software**: LLMs podem ser integrados a ferramentas de desenvolvimento de software, como GitHub Copilot ou StarCoder, para auxiliar os programadores, acelerar a gera√ß√£o de c√≥digo e melhorar a qualidade do desenvolvimento de software.\n",
      "5. **Aplicativos de escrit√≥rio**: Voc√™ pode integrar LLMs a ferramentas de escrit√≥rio, como Microsoft 365 Copilot ou Google Workspace, para transformar a efici√™ncia e a capacidade das empresas.\n",
      "\n",
      "Com LangChain, voc√™ pode criar aplica√ß√µes que combinam LLMs com outras tecnologias, como blockchain, para criar solu√ß√µes inovadoras e seguras para uma variedade de setores. Alguns exemplos incluem:\n",
      "\n",
      "1. **Plataformas de aprendizado**: Voc√™ pode criar plataformas de aprendizado que usam LLMs para gerar conte√∫do personalizado e adaptativo para os usu√°rios.\n",
      "2. **Sistemas de recomenda√ß√£o**: LLMs podem ser usados para criar sistemas de recomenda√ß√£o que sugerem produtos ou servi√ßos com base nas prefer√™ncias e comportamentos dos usu√°rios.\n",
      "3. **Ferramentas de colabora√ß√£o**: Voc√™ pode criar ferramentas de colabora√ß√£o que usam LLMs para ajudar as equipes a trabalhar juntas de forma mais eficiente e eficaz.\n",
      "\n",
      "Essas s√£o apenas algumas das principais aplica√ß√µes que voc√™ pode construir com LLM e LangChain. A combina√ß√£o dessas tecnologias oferece um vasto potencial para criar solu√ß√µes inovadoras e transformadoras em uma variedade de setores.\n",
      "=== Calling Function ===\n",
      "Calling function: article_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es que posso construir com LLM e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplica√ß√µes que voc√™ pode construir com LLMs incluem: \n",
      "\n",
      "1. Chatbots e assistentes virtuais para fornecer suporte ao cliente, solu√ß√£o de problemas ou conversas abertas.\n",
      "2. Gera√ß√£o de c√≥digo e depura√ß√£o para fornecer trechos √∫teis de c√≥digo como resposta a solicita√ß√µes escritas em linguagem natural.\n",
      "3. An√°lise de sentimento para ajudar a analisar emo√ß√µes e opini√µes a partir de um texto.\n",
      "4. Classifica√ß√£o e agrupamento de texto para categorizar e classificar grandes volumes de dados.\n",
      "5. Tradu√ß√£o de idiomas para globalizar todo o seu conte√∫do sem horas de trabalho √°rduo.\n",
      "6. Resumo e par√°fraseamento para resumir chamadas ou reuni√µes de clientes de forma eficiente.\n",
      "7. Gera√ß√£o de conte√∫do para desenvolver um esbo√ßo e criar ideias.\n",
      "\n",
      "Essas s√£o apenas algumas das aplica√ß√µes poss√≠veis, e as op√ß√µes podem variar dependendo das necessidades espec√≠ficas da sua organiza√ß√£o e dos recursos dispon√≠veis.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es que posso construir com LLM e LangChain?\"}\n"
     ]
    }
   ],
   "source": [
    "response = agent_document.chat(\n",
    "    \"Quais as principais aplica√ß√µes que posso construir com LLM e LangChain?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
