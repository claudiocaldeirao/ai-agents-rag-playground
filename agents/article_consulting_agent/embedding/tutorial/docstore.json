{"docstore/metadata": {"d0199156-0cef-449f-bd75-24640c930537": {"doc_hash": "17680f0de6aac9523553a3af57f1c24da39888774a00e770ceaaec17f337292c"}, "8b7fe797-6888-4f4a-af69-ee5bcee8de1b": {"doc_hash": "245ad1123172558818f2a3797d27f403ba64df9f9f3c476906f922e4a2d09b7e"}, "d45043cd-1d20-4559-9bb4-c6bf9f6f0f33": {"doc_hash": "7e418432cd6caf3b88ad7ca49cc6ed1f5a14b0295e33caa6286a8f4d35f9a4fc"}, "9c39d1b2-c70c-46be-93a0-0487312d6597": {"doc_hash": "15c4b12cf049edc317f599169556f77b8c5c35eb4ed4803335f62e1b7adb58a9"}, "2678d44b-7039-4edb-893b-42f1ba523e81": {"doc_hash": "c0e94130de1084c7a208d1f776549fc355c8f6c066eb405b293a2e1c5776463e"}, "3f4c349a-d20b-497c-8ffe-8a99519cd0fc": {"doc_hash": "2981185eec61893f3e8ca30abe00bce41b789314d5d4fa9a536989147b4c6ba5"}, "e6b8a447-7da3-46e4-a58e-3532ab371544": {"doc_hash": "228342c4c7ebfcc4a681211c2da78c731b495ce123d553c764ce55d625f2b916"}, "c5369d4d-074a-48c1-b379-dc99fa7775fc": {"doc_hash": "caff4dd036d02f430b4c26a0f11253932c3d9ca868483115581132717a72446b"}, "cf272f44-9567-426d-9001-c1599af129d3": {"doc_hash": "2995f7f73ea8e7cdfda0bf99ead80224e8ce878a160ae29f5e14eb03322a3322", "ref_doc_id": "d0199156-0cef-449f-bd75-24640c930537"}, "36f6ccf1-677e-4805-bdba-95c05a225d6b": {"doc_hash": "9062d96c959677a8449945c9ae48fc5a83d642028f9ed5885cdea09b04581541", "ref_doc_id": "8b7fe797-6888-4f4a-af69-ee5bcee8de1b"}, "70fedd76-d69d-4a10-bb07-922154f5cdaf": {"doc_hash": "2d85d251a610592df0f2c197f7607bfb66343ac7401afa7aafbc8c300d3bb915", "ref_doc_id": "8b7fe797-6888-4f4a-af69-ee5bcee8de1b"}, "307696df-3841-49a1-ac19-9e2519f335ab": {"doc_hash": "745bf1c2291e4dd8708967c9c8347a3198f8a79cc8b31cba800f6bb6b7acd531", "ref_doc_id": "d45043cd-1d20-4559-9bb4-c6bf9f6f0f33"}, "c3745170-f446-48be-81f5-e977731f6a49": {"doc_hash": "073557b7b489ce4ffe486d68a0686ca1fd92654e557fba415f3d4893ac6af001", "ref_doc_id": "d45043cd-1d20-4559-9bb4-c6bf9f6f0f33"}, "e4232058-b208-477c-b22c-7b3e2a8a558d": {"doc_hash": "a0dfb80f6590d98e0e59cae860f382b458a449e03b2052178ab724a6c739ac79", "ref_doc_id": "9c39d1b2-c70c-46be-93a0-0487312d6597"}, "8db86119-c30b-48fd-b457-c5dbd6203a5f": {"doc_hash": "ee831f8f76c4b0c50b50230a33547973994c72610e03f17902d52df2f6f9c6c2", "ref_doc_id": "2678d44b-7039-4edb-893b-42f1ba523e81"}, "8cfc1a4e-d2e7-476a-8154-43496913601c": {"doc_hash": "04045dafb5fbd29795d87a6371ef24ed79be351decf81a05b8d2711f1e8ea0bf", "ref_doc_id": "3f4c349a-d20b-497c-8ffe-8a99519cd0fc"}, "6e5a57e1-8b38-4ffa-9c17-f38566cff29c": {"doc_hash": "bfcbd65bad28002d0a31f92868557036a7cb0590260acab6a2691f46069cb4ca", "ref_doc_id": "3f4c349a-d20b-497c-8ffe-8a99519cd0fc"}, "f8fd9bb9-a80a-4369-ae94-74e2f8c2e655": {"doc_hash": "b1c7db71e6f92ee2fe1375850345d418855f26eec34a28c60220615e49037acb", "ref_doc_id": "e6b8a447-7da3-46e4-a58e-3532ab371544"}, "cd99749c-6667-4f54-98c0-c6c91a8d1785": {"doc_hash": "b0051a0cc8ea5eb6e0fd1eaeb142378069c1b63c1a9ec9620238fdeff5f021c8", "ref_doc_id": "c5369d4d-074a-48c1-b379-dc99fa7775fc"}}, "docstore/data": {"cf272f44-9567-426d-9001-c1599af129d3": {"__data__": {"id_": "cf272f44-9567-426d-9001-c1599af129d3", "embedding": null, "metadata": {"page_label": "1", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d0199156-0cef-449f-bd75-24640c930537", "node_type": "4", "metadata": {"page_label": "1", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "hash": "17680f0de6aac9523553a3af57f1c24da39888774a00e770ceaaec17f337292c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "MANAGEMENT SOLUTIONSA ascens\u00e3o dos Large Language Models: dos fundamentos \u00e0 aplica\u00e7\u00e3o\n14\nLLM: defini\u00e7\u00e3o, contexto e regula\u00e7\u00e3o\n\u201cMe disseram que eu teria um impacto positivo no mundo. Ningu\u00e9m me preparou para \na quantidade de perguntas rid\u00edculas que me fariam diariamente\u201c. \nAnthropic Claude25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 291, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "36f6ccf1-677e-4805-bdba-95c05a225d6b": {"__data__": {"id_": "36f6ccf1-677e-4805-bdba-95c05a225d6b", "embedding": null, "metadata": {"page_label": "2", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b7fe797-6888-4f4a-af69-ee5bcee8de1b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "hash": "245ad1123172558818f2a3797d27f403ba64df9f9f3c476906f922e4a2d09b7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70fedd76-d69d-4a10-bb07-922154f5cdaf", "node_type": "1", "metadata": {}, "hash": "79de909a628a425024fdb73aeb27547073178cef10ffaa0ab56198a0f61cc6c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15\nDefini\u00e7\u00e3o \nA Intelig\u00eancia Artificial Generativa (GenAI) \u00e9 um tipo de IA capaz \nde gerar v\u00e1rios tipos de conte\u00fado, como texto, imagens, v\u00eddeos \ne \u00e1udio. Ela usa modelos para aprender os padr\u00f5es e a estrutura \ndos dados de treinamento de entrada e, em seguida, gera novo \nconte\u00fado com base nesse conhecimento aprendido. \nDentro da GenAI, os modelos de linguagem de grande escala \n(LLM) s\u00e3o, de acordo com a Comiss\u00e3o Europeia, \"um tipo de \nmodelo de intelig\u00eancia artificial que foi treinado por algoritmos \nde aprendizagem profunda para reconhecer, gerar, traduzir \ne/ou resumir grandes quantidades de linguagem humana \nescrita e dados textuais\"26. \nMais comumente, esses modelos usam arquiteturas conhecidas \ncomo \"transformers\" que lhes permitem entender contextos \ncomplexos e capturar rela\u00e7\u00f5es entre palavras distantes no texto. \nTreinados em vastos conjuntos de dados, como livros, artigos e \np\u00e1ginas da Web, os LLMs aprendem padr\u00f5es e estruturas \nlingu\u00edsticas para executar uma variedade de tarefas, incluindo \ngera\u00e7\u00e3o de texto, tradu\u00e7\u00e3o e an\u00e1lise de sentimentos. \nA efic\u00e1cia de um LLM depende de seu tamanho, da diversidade \ndos dados de treinamento e da sofistica\u00e7\u00e3o de seus algoritmos, \no que influencia diretamente sua capacidade de aplica\u00e7\u00f5es \npr\u00e1ticas em v\u00e1rios campos. Portanto, o treinamento de um LLM \n\u00e9 uma tarefa que exige uma capacidade computacional e um \ntempo de m\u00e1quina muito altos e, portanto, custos muito \nsignificativos. Para refer\u00eancia, de acordo com Sam Altman, o \ntreinamento do GPT-4 custou \"mais de US$ 100 milh\u00f5es\"27. \nEsses altos custos significam que o desenvolvimento dos \nmaiores LLMs est\u00e1 concentrado em poucas organiza\u00e7\u00f5es em \ntodo o mundo (Fig. 4), com os recursos tecnol\u00f3gicos, cient\u00edficos \ne de investimento para lidar com projetos dessa escala. \n \nEvolu\u00e7\u00e3o dos LLMs \nO desenvolvimento dos LLMs representa uma evolu\u00e7\u00e3o \nsubstancial no campo do processamento de linguagem natural \n(NLP), que remonta ao trabalho fundamental sobre sem\u00e2ntica28 \nde Michel Br\u00e9al em 1883. O advento dos LLMs come\u00e7ou em \nmeados do s\u00e9culo XX, precedido por sistemas que dependiam \nmuito de regras gramaticais criadas manualmente. Um caso \nemblem\u00e1tico desse per\u00edodo \u00e9 o programa \"ELIZA\", criado em \n1966, que foi um avan\u00e7o ic\u00f4nico no desenvolvimento de \nmodelos de linguagem. \n\u00c0 medida que o campo evoluiu, as d\u00e9cadas de 1980 e 1990 \nviram uma mudan\u00e7a fundamental em dire\u00e7\u00e3o aos m\u00e9todos \nestat\u00edsticos de processamento de idiomas. Esse per\u00edodo viu a \nado\u00e7\u00e3o de Modelos Ocultos de Markov (HMMs) e modelos n-\ngram, que ofereceram uma abordagem mais din\u00e2mica para \nprever sequ\u00eancias de palavras com base em probabilidades, ao \ninv\u00e9s de sistemas de regras fixas. \nO ressurgimento das redes neurais no in\u00edcio dos anos 2000, \ngra\u00e7as aos avan\u00e7os nos algoritmos de retropropaga\u00e7\u00e3o que \nmelhoraram o treinamento de redes multicamadas, marcou um \ndesenvolvimento crucial. Um marco foi a introdu\u00e7\u00e3o de redes \nneurais de alimenta\u00e7\u00e3o direta para modelagem de linguagens29 \npor Bengio et al. em 2003. Isso estabeleceu a base para \ninova\u00e7\u00f5es subsequentes na representa\u00e7\u00e3o de palavras, \nprincipalmente a introdu\u00e7\u00e3o de embeddings de palavras30 por \nMikolov et al. em 2013 por meio do Word2Vec. Os embeddings \n \n25Claude (lan\u00e7ado em 2023) \u00e9 um modelo de linguagem treinado pela Anthropic, \numa startup de IA fundada por Dario Amodei, Daniela Amodei, Tom Brown, \nChris Olah, Sam McCandlish, Jack Clarke e Jared Kaplan em 2021.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3422, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70fedd76-d69d-4a10-bb07-922154f5cdaf": {"__data__": {"id_": "70fedd76-d69d-4a10-bb07-922154f5cdaf", "embedding": null, "metadata": {"page_label": "2", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b7fe797-6888-4f4a-af69-ee5bcee8de1b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "hash": "245ad1123172558818f2a3797d27f403ba64df9f9f3c476906f922e4a2d09b7e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36f6ccf1-677e-4805-bdba-95c05a225d6b", "node_type": "1", "metadata": {"page_label": "2", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "hash": "9062d96c959677a8449945c9ae48fc5a83d642028f9ed5885cdea09b04581541", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Um marco foi a introdu\u00e7\u00e3o de redes \nneurais de alimenta\u00e7\u00e3o direta para modelagem de linguagens29 \npor Bengio et al. em 2003. Isso estabeleceu a base para \ninova\u00e7\u00f5es subsequentes na representa\u00e7\u00e3o de palavras, \nprincipalmente a introdu\u00e7\u00e3o de embeddings de palavras30 por \nMikolov et al. em 2013 por meio do Word2Vec. Os embeddings \n \n25Claude (lan\u00e7ado em 2023) \u00e9 um modelo de linguagem treinado pela Anthropic, \numa startup de IA fundada por Dario Amodei, Daniela Amodei, Tom Brown, \nChris Olah, Sam McCandlish, Jack Clarke e Jared Kaplan em 2021. Claude foi \nprojetado usando a t\u00e9cnica de \"autoaprendizagem constitucionalmente \nalinhada\" da Anthropic, que se baseia em fornecer ao modelo uma lista de \nprinc\u00edpios e regras para aumentar sua seguran\u00e7a e evitar comportamentos \nprejudiciais. \n26European Commission (2024). \n27Wired (2023). \n28Br\u00e9al (1883). \n29Bengio (2003). \n30Mikolov (2013).", "mimetype": "text/plain", "start_char_idx": 2877, "end_char_idx": 3766, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "307696df-3841-49a1-ac19-9e2519f335ab": {"__data__": {"id_": "307696df-3841-49a1-ac19-9e2519f335ab", "embedding": null, "metadata": {"page_label": "3", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d45043cd-1d20-4559-9bb4-c6bf9f6f0f33", "node_type": "4", "metadata": {"page_label": "3", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "hash": "7e418432cd6caf3b88ad7ca49cc6ed1f5a14b0295e33caa6286a8f4d35f9a4fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3745170-f446-48be-81f5-e977731f6a49", "node_type": "1", "metadata": {}, "hash": "61c1ce63f27ecb68eb24ba0dc22a4b37bca7431594f7145f8622ece48ac15734", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "MANAGEMENT SOLUTIONSA ascens\u00e3o dos Large Language Models: dos fundamentos \u00e0 aplica\u00e7\u00e3o\n16\n \n31Parikh, A. P. (2016). \n32Vaswani (2017). \n33Euronews (2023). \n34Adaptado de MindsDB (2024) e ampliado. \nrepresentam as palavras como vetores de n\u00fameros e permitem \nque as dist\u00e2ncias entre as palavras sejam definidas, de modo \nque conceitos semelhantes tenham dist\u00e2ncias reduzidas, o que \npermite que as rela\u00e7\u00f5es sem\u00e2nticas sejam capturadas com uma \nefic\u00e1cia sem precedentes.   \nOs primeiros mecanismos de aten\u00e7\u00e3o foram introduzidos em \n201631, e permitiram resultados sem precedentes em tarefas de \nprocessamento de linguagem, pois identificaram a relev\u00e2ncia de \ndiferentes partes do texto de entrada. Mas foi a introdu\u00e7\u00e3o da \narquitetura \u201dtransformer\u201c32 por Vaswani et al. em 2017 que \nrepresentou a verdadeira mudan\u00e7a de paradigma no \ntreinamento de modelos e possibilitou o surgimento dos LLMs. \nA principal inova\u00e7\u00e3o dos transformers est\u00e1 nos mecanismos de \nautoaten\u00e7\u00e3o, que permitem que os modelos ponderem a \nimport\u00e2ncia relativa de diferentes palavras em uma frase. Isso \nsignifica que o modelo pode se concentrar nas partes mais \nrelevantes do texto ao gerar a resposta, o que \u00e9 fundamental \npara analisar o contexto e as rela\u00e7\u00f5es complexas dentro das \nsequ\u00eancias de palavras. Al\u00e9m disso, ao permitir o \nprocessamento paralelo de dados, os transformers melhoram a \nefici\u00eancia, a velocidade e o desempenho do treinamento do \nmodelo. \nA s\u00e9rie de modelos GPT desenvolvidos pela OpenAI, come\u00e7ando \ncom o GPT-1 em junho de 2018 e chegando ao GPT-4 em mar\u00e7o \nde 2023, exemplifica os r\u00e1pidos avan\u00e7os nos recursos dos LLMs. \nEm particular, o GPT-3, lan\u00e7ado em 2020 com 175 bilh\u00f5es de \npar\u00e2metros, alcan\u00e7ou o p\u00fablico em geral e mostrou o amplo \npotencial dos LLMs em v\u00e1rias aplica\u00e7\u00f5es. Al\u00e9m da s\u00e9rie GPT da \nOpenAI, outros modelos de LLM, como o Google Gemini e o \nAnthropic Claude, surgiram como participantes importantes no \ncen\u00e1rio da IA. O Gemini \u00e9 um exemplo de como as grandes \nempresas de tecnologia est\u00e3o investindo no desenvolvimento \nde LLMs avan\u00e7ados, enquanto o Claude representa um esfor\u00e7o \npara criar LLMs que n\u00e3o sejam apenas poderosos, mas tamb\u00e9m \nalinhados com princ\u00edpios \u00e9ticos e seguros para uso. \nO ano de 2023, apelidado de \u201do ano da IA\u201c33, se destaca como \num marco na hist\u00f3ria dos LLMs, caracterizado por maior \nacessibilidade e contribui\u00e7\u00f5es globais. As inova\u00e7\u00f5es durante \nesse ano demonstraram que os LLMs podem ser criados com o \nm\u00ednimo de c\u00f3digo, reduzindo significativamente as barreiras de \nentrada, ao mesmo tempo em que introduzem novos desafios, \ncomo o custo do treinamento e da infer\u00eancia, e seus riscos \nFigura 4. Alguns dos principais LLM e seus fornecedores34.\nEmpresa LLM Coment\u00e1rios Pa\u00eds\nOpenAI ChatGPT Conhecido por sua versatilidade em tarefas lingu\u00edsticas, popular para \npreenchimento de texto, tradu\u00e7\u00e3o e muito mais.\nEstados Unidos\nMicrosoft Orca Concentra-se na cria\u00e7\u00e3o de dados sint\u00e9ticos e em recursos de \nracioc\u00ednio aprimorados.\nEstados Unidos\nAnthropic Claude Reconhecido por seu amplo conhecimento geral e recursos \nmultil\u00edngues.\nEstados Unidos\nGoogle Gemini, Gemma, BERT Pioneira no processamento de idiomas com modelos que suportam \nv\u00e1rios tipos de dados.\nEstados Unidos\nMeta AI Llama Conhecida pela efici\u00eancia e pelo acesso democratizado, com foco no \nalto desempenho com computa\u00e7\u00e3o reduzida.\nEstados Unidos\nLMSYS Vicuna Ajustado para funcionalidades de chatbot, oferecendo uma \nabordagem exclusiva para intera\u00e7\u00f5es de conversa\u00e7\u00e3o.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3473, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3745170-f446-48be-81f5-e977731f6a49": {"__data__": {"id_": "c3745170-f446-48be-81f5-e977731f6a49", "embedding": null, "metadata": {"page_label": "3", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d45043cd-1d20-4559-9bb4-c6bf9f6f0f33", "node_type": "4", "metadata": {"page_label": "3", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "hash": "7e418432cd6caf3b88ad7ca49cc6ed1f5a14b0295e33caa6286a8f4d35f9a4fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "307696df-3841-49a1-ac19-9e2519f335ab", "node_type": "1", "metadata": {"page_label": "3", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "hash": "745bf1c2291e4dd8708967c9c8347a3198f8a79cc8b31cba800f6bb6b7acd531", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Estados Unidos\nMicrosoft Orca Concentra-se na cria\u00e7\u00e3o de dados sint\u00e9ticos e em recursos de \nracioc\u00ednio aprimorados.\nEstados Unidos\nAnthropic Claude Reconhecido por seu amplo conhecimento geral e recursos \nmultil\u00edngues.\nEstados Unidos\nGoogle Gemini, Gemma, BERT Pioneira no processamento de idiomas com modelos que suportam \nv\u00e1rios tipos de dados.\nEstados Unidos\nMeta AI Llama Conhecida pela efici\u00eancia e pelo acesso democratizado, com foco no \nalto desempenho com computa\u00e7\u00e3o reduzida.\nEstados Unidos\nLMSYS Vicuna Ajustado para funcionalidades de chatbot, oferecendo uma \nabordagem exclusiva para intera\u00e7\u00f5es de conversa\u00e7\u00e3o.\nEstados Unidos\nCohere Command-nightly Especializada em tempos de resposta r\u00e1pidos e pesquisa sem\u00e2ntica \nem mais de 100 idiomas.\nCanad\u00e1\nMistral AI Mistral, Mixtral Enfatiza modelos menores, mas poderosos, operando localmente com \nm\u00e9tricas de desempenho s\u00f3lidas.\nFrancia\nClibrain LINCE Adaptado para o idioma espanhol, com foco em nuances lingu\u00edsticas e \ncompreens\u00e3o de qualidade.\nEspa\u00f1a\nTechnology \nInnovation Institute\nFalcon Fornece modelos de IA de c\u00f3digo aberto altamente eficientes e \ndimension\u00e1veis com suporte multil\u00edngue.\nEmiratos \u00c1rabes Unidos\nAleph Alpha Luminous Not\u00e1vel por sua abordagem multimodal e desempenho competitivo \nnas principais tarefas de IA.\nAlemania \nSenseTime SenseNova Uma s\u00e9rie de modelos e aplicativos de IA generativa que fazem uso da \nplataforma de pesquisa e desenvolvimento da AGI e integram LLMs \ncom sistemas de computa\u00e7\u00e3o em larga escala (SenseCore, com 5.000 \npetaflops). \nHong Kong", "mimetype": "text/plain", "start_char_idx": 2851, "end_char_idx": 4393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e4232058-b208-477c-b22c-7b3e2a8a558d": {"__data__": {"id_": "e4232058-b208-477c-b22c-7b3e2a8a558d", "embedding": null, "metadata": {"page_label": "4", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9c39d1b2-c70c-46be-93a0-0487312d6597", "node_type": "4", "metadata": {"page_label": "4", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "hash": "15c4b12cf049edc317f599169556f77b8c5c35eb4ed4803335f62e1b7adb58a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17\ninerentes. Nesse per\u00edodo, tamb\u00e9m houve uma preocupa\u00e7\u00e3o \ncrescente com as considera\u00e7\u00f5es e os desafios \u00e9ticos \napresentados pelo desenvolvimento e uso de LLMs e, como \nconsequ\u00eancia, um avan\u00e7o na regulamenta\u00e7\u00e3o da IA e da IA \ngenerativa em todo o mundo. \nA prolifera\u00e7\u00e3o de LLMs de c\u00f3digo aberto foi um marco na \ndemocratiza\u00e7\u00e3o da tecnologia de IA. Come\u00e7ando com o Llama e \ncontinuando com Vicuna, Falcon, Mistral, Gemma e outros, os \nLLMs de c\u00f3digo aberto democratizaram o acesso \u00e0 tecnologia \nde ponta de processamento de linguagem e permitiram que \npesquisadores, desenvolvedores e amadores experimentassem, \npersonalizassem e implantassem solu\u00e7\u00f5es de IA com um \ninvestimento inicial m\u00ednimo. A disponibilidade desses modelos \npromoveu uma colabora\u00e7\u00e3o sem precedentes na comunidade \nde IA, estimulando a inova\u00e7\u00e3o e facilitando a cria\u00e7\u00e3o de \naplicativos avan\u00e7ados em diversos setores. \nPor fim, a integra\u00e7\u00e3o do LLM \u00e0s ferramentas de \ndesenvolvimento de software e de escrit\u00f3rio est\u00e1 \ntransformando a efici\u00eancia e a capacidade das empresas. A \nMicrosoft integrou o LLM em seu pacote Office com o nome \nMicrosoft 365 Copilot, enquanto o Google fez o mesmo no \nGoogle Workspace. Ao mesmo tempo, ferramentas como o \nGitHub Copilot ou o StarCoder usam LLM para auxiliar os \nprogramadores, acelerando a gera\u00e7\u00e3o de c\u00f3digo e melhorando \na qualidade do desenvolvimento de software. \n \nTipologias de LLM \nOs LLMs progrediram al\u00e9m da simples previs\u00e3o de texto e se \ntornaram aplicativos sofisticados em v\u00e1rios dom\u00ednios, \narquiteturas e modalidades. Esta se\u00e7\u00e3o apresenta uma \ncategoriza\u00e7\u00e3o dos LLMs de acordo com v\u00e1rios crit\u00e9rios. \nPor arquitetura \n4 LLMs baseados em redes neurais recorrentes (RNNs): \nesses modelos processam o texto sequencialmente, \nanalisando o impacto de cada palavra sobre a pr\u00f3xima, e \nusam arquiteturas recorrentes, como mem\u00f3ria de longo \nprazo (LSTM) ou unidades de passagem recorrentes (GRU), \npara processar dados sequenciais. Embora n\u00e3o sejam t\u00e3o \neficientes quanto os transformers para sequ\u00eancias longas, os \nRNNs s\u00e3o \u00fateis para tarefas em que a compreens\u00e3o da \nordem das palavras \u00e9 crucial, como na tradu\u00e7\u00e3o autom\u00e1tica. \nExemplos s\u00e3o o ELMo (Embeddings from Language Models) \ne o ULMFiT (Universal Language Model Fine-tuning). \n4 LLMs baseados en transformers: essa \u00e9 a arquitetura \ndominante para LLMs atualmente. Eles usam transformers \npara analisar as rela\u00e7\u00f5es entre as palavras em uma frase. Isso \npermite que eles capturem estruturas gramaticais \ncomplexas e depend\u00eancias de palavras com longa dist\u00e2ncia. \nA maioria dos LLMs, como GPT, Claude e Gemini, pertence a \nessa categoria. \nPor componente \n4 Codificadores (Encoders): s\u00e3o modelos projetados para \nentender (codificar) as informa\u00e7\u00f5es de entrada. Eles \ntransformam o texto em uma representa\u00e7\u00e3o vetorial, \ncapturando seu significado sem\u00e2ntico. Os encoders s\u00e3o \nfundamentais em tarefas como a compreens\u00e3o e a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2887, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8db86119-c30b-48fd-b457-c5dbd6203a5f": {"__data__": {"id_": "8db86119-c30b-48fd-b457-c5dbd6203a5f", "embedding": null, "metadata": {"page_label": "5", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2678d44b-7039-4edb-893b-42f1ba523e81", "node_type": "4", "metadata": {"page_label": "5", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "hash": "c0e94130de1084c7a208d1f776549fc355c8f6c066eb405b293a2e1c5776463e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "MANAGEMENT SOLUTIONSA ascens\u00e3o dos Large Language Models: dos fundamentos \u00e0 aplica\u00e7\u00e3o\n18\nclassifica\u00e7\u00e3o de textos. Um exemplo \u00e9 o BERT do Google, um \nmodelo que analisa o contexto de cada palavra em um texto \npara entender seu significado completo, e que n\u00e3o \u00e9 \nrealmente um LLM. \n4 Decodificadores (Decoders): esses modelos geram \n(decodificam) texto a partir de representa\u00e7\u00f5es vetoriais. Eles \ns\u00e3o essenciais na gera\u00e7\u00e3o de texto, como na cria\u00e7\u00e3o de novo \nconte\u00fado a partir de prompts fornecidos. A maioria dos \nLLMs s\u00e3o decodificadores. \n4 Codificadores/Decodificadores (Encoders/Decoders): \nesses modelos combinam encoders e decoders para \nconverter um tipo de informa\u00e7\u00e3o em outro, facilitando \ntarefas como a tradu\u00e7\u00e3o autom\u00e1tica, em que o texto de \nentrada \u00e9 codificado e depois decodificado em outro \nidioma. Um exemplo \u00e9 o T5 (Text-to-Text Transfer \nTransformer) do Google, projetado para lidar com v\u00e1rias \ntarefas de processamento de linguagem natural. \nPor abordagem de treinamento \n4 LLM pr\u00e9-treinados: esses modelos s\u00e3o primeiramente \ntreinados em um grande corpus de texto n\u00e3o rotulado \nusando t\u00e9cnicas de aprendizagem auto-supervisionadas, \ncomo modelagem de linguagem mascarada ou previs\u00e3o da \npr\u00f3xima frase, e podem ser ajustados com dados rotulados \nmenores para tarefas espec\u00edficas. Os exemplos incluem \nmodelos como GPT, Mistral, BERT e RoBERTa, entre muitos \noutros. \n4 LLM espec\u00edficos: esses modelos s\u00e3o treinados do zero com \ndados rotulados para uma tarefa espec\u00edfica, como an\u00e1lise de \nsentimentos, resumo de texto ou tradu\u00e7\u00e3o autom\u00e1tica. Os \nexemplos incluem modelos de tradu\u00e7\u00e3o e resumo. \nPor modalidade \n4 LLM somente de texto: s\u00e3o o tipo mais comum, treinados \ne trabalhando exclusivamente com dados textuais. \nExemplos s\u00e3o GPT-3, Mistral ou Gemma. \n4 LLM multimodais: \u00e9 um campo emergente em que os \nLLMs s\u00e3o treinados em uma combina\u00e7\u00e3o de texto e outros \nformatos de dados, como imagens ou \u00e1udio. Isso permite \nque eles executem tarefas que exigem a compreens\u00e3o da \nrela\u00e7\u00e3o entre diferentes modalidades. Exemplos s\u00e3o GPT-4, \nClaude 3 e Gemini. \nPor tamanho \n4 Large language models (LLM): s\u00e3o modelos que usam \ngrandes quantidades de par\u00e2metros. Eles s\u00e3o muito \navan\u00e7ados, mas exigem uma infraestrutura tecnol\u00f3gica \nrelativamente cara na nuvem para sua execu\u00e7\u00e3o. Exemplos \ns\u00e3o o GPT-4, o Gemini e o Claude 3. \n4 Small language models (SLM): uma tend\u00eancia recente, os \nSLMs s\u00e3o vers\u00f5es menores e mais eficientes dos LLMs, \nprojetados para serem executados em dispositivos com \nrecursos limitados, como smartphones ou dispositivos de \nIoT, sem a necessidade de conex\u00e3o ou implanta\u00e7\u00e3o na \nnuvem. Apesar de seu tamanho pequeno, esses modelos \nmant\u00eam um desempenho aceit\u00e1vel gra\u00e7as a t\u00e9cnicas como \ncompress\u00e3o ou quantiza\u00e7\u00e3o de modelos, o que reduz a \nprecis\u00e3o dos pesos e ativa\u00e7\u00f5es do modelo. Exemplos s\u00e3o o \nGemini Nano do Google e a fam\u00edlia de modelos Phi da \nMicrosoft.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2894, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8cfc1a4e-d2e7-476a-8154-43496913601c": {"__data__": {"id_": "8cfc1a4e-d2e7-476a-8154-43496913601c", "embedding": null, "metadata": {"page_label": "6", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f4c349a-d20b-497c-8ffe-8a99519cd0fc", "node_type": "4", "metadata": {"page_label": "6", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "hash": "2981185eec61893f3e8ca30abe00bce41b789314d5d4fa9a536989147b4c6ba5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e5a57e1-8b38-4ffa-9c17-f38566cff29c", "node_type": "1", "metadata": {}, "hash": "756b3b7c91fefdfa9a8db1dd4a9acd3435494889bd967b93860c2bdbc49ff0e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "LLMs na pr\u00e1tica: casos de uso em produ\u00e7\u00e3o \n19\nApesar do crescente interesse e da explora\u00e7\u00e3o de poss\u00edveis \naplica\u00e7\u00f5es do LLM nas organiza\u00e7\u00f5es, os casos de uso reais \nimplementados em produ\u00e7\u00e3o ainda s\u00e3o limitados. A maioria das \nempresas est\u00e1 em um est\u00e1gio relativamente inicial, identificando e \npriorizando poss\u00edveis casos de uso. \nNo entanto, v\u00e1rias empresas j\u00e1 conseguiram colocar alguns casos \nde LLM em produ\u00e7\u00e3o, demonstrando seu valor tang\u00edvel para a \nempresa e seus clientes. Alguns desses casos est\u00e3o resumidos aqui: \n4 Chatbots internos: v\u00e1rias organiza\u00e7\u00f5es implementaram \nchatbots baseados em LLM para facilitar o acesso dos \nfuncion\u00e1rios a pol\u00edticas, procedimentos e informa\u00e7\u00f5es \nrelevantes da empresa. Esses assistentes de conversa\u00e7\u00e3o \npermitem respostas r\u00e1pidas e precisas a consultas frequentes, \nmelhorando a efici\u00eancia e reduzindo a carga sobre outros \ncanais de suporte interno. \n4 Extra\u00e7\u00e3o de informa\u00e7\u00f5es: os LLMs est\u00e3o sendo usados para \nextrair automaticamente dados importantes de documentos \ngrandes e complexos, como relat\u00f3rios anuais ou relat\u00f3rios de \nrisco clim\u00e1tico. Essas ferramentas s\u00e3o capazes de processar \narquivos PDF de milhares de p\u00e1ginas, com estruturas \nheterog\u00eaneas, incluindo imagens, gr\u00e1ficos e tabelas, e \ntransformar as informa\u00e7\u00f5es relevantes em formatos \nestruturados e acess\u00edveis, como tabelas ordenadas. Essa \nautoma\u00e7\u00e3o permite que as empresas economizem tempo e \nrecursos em tarefas de an\u00e1lise de documentos. \n4 Suporte ao centro de atendimento ao cliente: alguns contact \ncenters  est\u00e3o aproveitando os LLMs para melhorar a \nqualidade e a efici\u00eancia do servi\u00e7o. Ao aplicar t\u00e9cnicas de \ntranscri\u00e7\u00e3o e resumo, essas ferramentas geram um contexto \ndas intera\u00e7\u00f5es anteriores de cada cliente, permitindo que os \nagentes ofere\u00e7am um servi\u00e7o mais personalizado. Al\u00e9m disso, \ndurante as chamadas em andamento, os LLMs podem fornecer \naos agentes acesso em tempo real \u00e0 documenta\u00e7\u00e3o relevante \npara responder a consultas espec\u00edficas dos clientes, como \ninforma\u00e7\u00f5es sobre taxas banc\u00e1rias ou instru\u00e7\u00f5es para bloqueio \nde cart\u00f5es de cr\u00e9dito. \n4 Classifica\u00e7\u00e3o inteligente de documentos: os recursos de \nprocessamento de linguagem natural dos LLMs est\u00e3o sendo \naplicados para classificar automaticamente grandes volumes de \ndocumentos, como contratos ou faturas, com base em seu \nconte\u00fado. Essa categoriza\u00e7\u00e3o inteligente permite que as \norganiza\u00e7\u00f5es otimizem os processos de gest\u00e3o de documentos e \nfacilita a busca e a recupera\u00e7\u00e3o de informa\u00e7\u00f5es relevantes. \n4 Banco conversacional: alguns bancos est\u00e3o integrando o LLM \nem seus aplicativos m\u00f3veis e canais digitais para oferecer \nexperi\u00eancias avan\u00e7adas de conversa\u00e7\u00e3o aos seus clientes. Esses \nchatbots s\u00e3o capazes de acessar os dados transacionais dos \nusu\u00e1rios em tempo real e responder a consultas espec\u00edficas, \ncomo \u201dComo foram meus gastos no \u00faltimo m\u00eas?\u201d ou \u201dQuanto \nganhei de juros em meus dep\u00f3sitos no \u00faltimo ano?\u201d \n4 Assist\u00eancia na elabora\u00e7\u00e3o de relat\u00f3rios de auditoria: as \nfun\u00e7\u00f5es de auditoria interna de algumas empresas j\u00e1 est\u00e3o \nusando o LLM para simplificar seus relat\u00f3rios. Essas \nferramentas utilizam como insumos as conclus\u00f5es do auditor, \num banco de dados de relat\u00f3rios anteriores e um banco de \ndados de regulamentos internos e externos aplic\u00e1veis. A partir \ndessas informa\u00e7\u00f5es, os LLMs geram um rascunho avan\u00e7ado do \nrelat\u00f3rio de auditoria, adotando o tom, o vocabul\u00e1rio e o estilo \ndos auditores humanos e citando adequadamente os relat\u00f3rios \nanteriores e as regulamenta\u00e7\u00f5es relevantes. Isso permite que os \nauditores economizem muito tempo em tarefas de reda\u00e7\u00e3o e se \nconcentrem em atividades de maior valor agregado.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3623, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e5a57e1-8b38-4ffa-9c17-f38566cff29c": {"__data__": {"id_": "6e5a57e1-8b38-4ffa-9c17-f38566cff29c", "embedding": null, "metadata": {"page_label": "6", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f4c349a-d20b-497c-8ffe-8a99519cd0fc", "node_type": "4", "metadata": {"page_label": "6", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "hash": "2981185eec61893f3e8ca30abe00bce41b789314d5d4fa9a536989147b4c6ba5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8cfc1a4e-d2e7-476a-8154-43496913601c", "node_type": "1", "metadata": {"page_label": "6", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "hash": "04045dafb5fbd29795d87a6371ef24ed79be351decf81a05b8d2711f1e8ea0bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Essas \nferramentas utilizam como insumos as conclus\u00f5es do auditor, \num banco de dados de relat\u00f3rios anteriores e um banco de \ndados de regulamentos internos e externos aplic\u00e1veis. A partir \ndessas informa\u00e7\u00f5es, os LLMs geram um rascunho avan\u00e7ado do \nrelat\u00f3rio de auditoria, adotando o tom, o vocabul\u00e1rio e o estilo \ndos auditores humanos e citando adequadamente os relat\u00f3rios \nanteriores e as regulamenta\u00e7\u00f5es relevantes. Isso permite que os \nauditores economizem muito tempo em tarefas de reda\u00e7\u00e3o e se \nconcentrem em atividades de maior valor agregado. \nEsses exemplos ilustram como os LLMs est\u00e3o criando valor real em \numa variedade de fun\u00e7\u00f5es de neg\u00f3cios, desde a otimiza\u00e7\u00e3o de \nprocessos internos at\u00e9 a melhoria da experi\u00eancia do cliente. Embora \no n\u00famero de casos de uso em produ\u00e7\u00e3o seja atualmente limitado, \nespera-se que essa tend\u00eancia se acelere muito rapidamente em um \nfuturo pr\u00f3ximo, \u00e0 medida que os LLMs continuem a evoluir e os \ndesafios relacionados \u00e0 privacidade e \u00e0 seguran\u00e7a dos dados sejam \ntratados de forma eficaz.", "mimetype": "text/plain", "start_char_idx": 3072, "end_char_idx": 4105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8fd9bb9-a80a-4369-ae94-74e2f8c2e655": {"__data__": {"id_": "f8fd9bb9-a80a-4369-ae94-74e2f8c2e655", "embedding": null, "metadata": {"page_label": "7", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6b8a447-7da3-46e4-a58e-3532ab371544", "node_type": "4", "metadata": {"page_label": "7", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "hash": "228342c4c7ebfcc4a681211c2da78c731b495ce123d553c764ce55d625f2b916", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "MANAGEMENT SOLUTIONSA ascens\u00e3o dos Large Language Models: dos fundamentos \u00e0 aplica\u00e7\u00e3o\n20\nPrincipais usos \nOs LLMs est\u00e3o encontrando aplica\u00e7\u00f5es em uma infinidade de \ndom\u00ednios, transformando substancialmente a maneira como as \npessoas interagem com a tecnologia e aproveitam o \nprocessamento de linguagem natural para aprimorar processos, \nservi\u00e7os e experi\u00eancias. \nAlguns dos usos mais proeminentes dos LLMs de texto est\u00e3o \nresumidos abaixo. \n1. Cria\u00e7\u00e3o e aprimoramento de conte\u00fado \n4Gera\u00e7\u00e3o de conte\u00fado: produ\u00e7\u00e3o autom\u00e1tica de texto. \n4Assist\u00eancia na reda\u00e7\u00e3o: corre\u00e7\u00e3o ortogr\u00e1fica, de estilo e \nde conte\u00fado. \n4Tradu\u00e7\u00e3o autom\u00e1tica: convers\u00e3o de texto de um \nidioma para outro. \n4Resumo de textos: redu\u00e7\u00e3o de documentos longos em \nresumos. \n4Planejamento e roteiro de conte\u00fado: estrutura\u00e7\u00e3o do \nconte\u00fado, p. ex., \u00edndice. \n4Brainstorming: propostas criativas para projetos, \nnomes, conceitos, etc. \n4Programa\u00e7\u00e3o: cria\u00e7\u00e3o de c\u00f3digo de programa\u00e7\u00e3o a \npartir de linguagem natural. \n \n2. An\u00e1lise e organiza\u00e7\u00e3o de informa\u00e7\u00f5es \n4An\u00e1lise de sentimento: avalia\u00e7\u00e3o de emo\u00e7\u00f5es e \nopini\u00f5es em textos. \n4Extra\u00e7\u00e3o de informa\u00e7\u00f5es: extra\u00e7\u00e3o de dados espec\u00edficos \nde documentos grandes. \n4Classifica\u00e7\u00e3o de textos: organiza\u00e7\u00e3o de textos em \ncategorias ou temas espec\u00edficos. \n4Revis\u00e3o t\u00e9cnica: assist\u00eancia na revis\u00e3o de documentos \nespecializados (por exemplo, jur\u00eddicos). \n3. Intera\u00e7\u00e3o e automa\u00e7\u00e3o \n4Chatbots: simula\u00e7\u00e3o de conversas sobre t\u00f3picos gerais \nou espec\u00edficos. \n4Perguntas e respostas: gera\u00e7\u00e3o de respostas a \nperguntas com base em um corpus. \n \nEsses usos resumem as aplica\u00e7\u00f5es atuais dos LLMs de texto. \nCom o surgimento dos LLMs multimodais, outras aplica\u00e7\u00f5es \nest\u00e3o come\u00e7ando a surgir, como a gera\u00e7\u00e3o de conte\u00fado \naudiovisual, a interpreta\u00e7\u00e3o de dados de imagens, a tradu\u00e7\u00e3o \nde conte\u00fado multim\u00eddia ou a cria\u00e7\u00e3o de experi\u00eancias interativas \nricas, como a intera\u00e7\u00e3o com chatbots com entrada n\u00e3o apenas \nde texto, mas tamb\u00e9m de imagem, \u00e1udio e v\u00eddeo. \nRequisitos regulat\u00f3rios \nA r\u00e1pida evolu\u00e7\u00e3o da intelig\u00eancia artificial generativa, \nespecialmente no campo da modelagem de linguagem de \nlarga escala (LLM), chamou a aten\u00e7\u00e3o dos \u00f3rg\u00e3os reguladores \nem todo o mundo. O potencial desses sistemas de influenciar \nnegativamente os cidad\u00e3os levou ao aumento das iniciativas \npara estabelecer marcos regulat\u00f3rios para garantir seu \ndesenvolvimento e uso respons\u00e1vel. \nAlgumas das principais iniciativas regulat\u00f3rias sobre IA incluem: \n4 O AI Act da Uni\u00e3o Europeia: uma proposta legislativa \npioneira para regulamentar a IA, que classifica os sistemas \nde IA de acordo com seu n\u00edvel de risco e estabelece \nrequisitos de transpar\u00eancia, seguran\u00e7a e direitos \nfundamentais. O AI Act foi adotado pelo Parlamento \nEuropeu em 13 de mar\u00e7o de 2024. \n4 O AI Bill of Rights dos EUA: um documento de orienta\u00e7\u00e3o \nque busca proteger os direitos civis no desenvolvimento e \nna aplica\u00e7\u00e3o da IA, enfatizando a privacidade, a n\u00e3o \ndiscrimina\u00e7\u00e3o e a transpar\u00eancia. \n4 O guia sobre IA do NIST dos EUA35 : estabelece princ\u00edpios \npara a cria\u00e7\u00e3o de sistemas de IA confi\u00e1veis, com foco na \nprecis\u00e3o, explicabilidade e mitiga\u00e7\u00e3o de vieses. \n4 A Declara\u00e7\u00e3o de Bletchley: compromisso internacional \ncom o desenvolvimento respons\u00e1vel da IA, promovendo \nprinc\u00edpios de transpar\u00eancia, seguran\u00e7a e imparcialidade, \nassinado por v\u00e1rios pa\u00edses. \n \n35O National Institute of Standards and Technology (NIST) publicou documentos \ndetalhando estruturas para seguran\u00e7a cibern\u00e9tica, gest\u00e3o de riscos e, \nespecificamente, gest\u00e3o de modelos de IA e IA generativa.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3507, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd99749c-6667-4f54-98c0-c6c91a8d1785": {"__data__": {"id_": "cd99749c-6667-4f54-98c0-c6c91a8d1785", "embedding": null, "metadata": {"page_label": "8", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5369d4d-074a-48c1-b379-dc99fa7775fc", "node_type": "4", "metadata": {"page_label": "8", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}, "hash": "caff4dd036d02f430b4c26a0f11253932c3d9ca868483115581132717a72446b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "21\n \n36IAPP (2024). \n \nAl\u00e9m das iniciativas acima, v\u00e1rios pa\u00edses come\u00e7aram a emitir \nsuas pr\u00f3prias regula\u00e7\u00f5es locais ou estabeleceram princ\u00edpios \npara a ado\u00e7\u00e3o segura e \u00e9tica da IA. Esses pa\u00edses incluem36 Reino \nUnido, Fran\u00e7a, Espanha, Alemanha, Holanda, Pol\u00f4nia, Austr\u00e1lia, \nNova Zel\u00e2ndia, Cingapura, Canad\u00e1, Jap\u00e3o, Coreia do Sul, China, \n\u00cdndia, Indon\u00e9sia, Israel, Emirados \u00c1rabes Unidos, Ar\u00e1bia Saudita, \nEgito, Brasil, Chile, Peru, Argentina, M\u00e9xico, Col\u00f4mbia e Turquia, \nentre outros. \nTodas essas iniciativas regulat\u00f3rias t\u00eam requisitos muito \nsemelhantes sobre IA que, quando aplicados aos LLMs, podem \nser resumidos da seguinte forma: \n4 Transpar\u00eancia e explicabilidade: obriga\u00e7\u00e3o de divulgar \ncomo o LLM funciona, incluindo a l\u00f3gica por tr\u00e1s de seus \nresultados, de modo que sejam compreens\u00edveis para os \nusu\u00e1rios. \n4 Privacidade e prote\u00e7\u00e3o de dados: medidas rigorosas para \nproteger as informa\u00e7\u00f5es pessoais coletadas ou geradas pelo \nLLM, em conformidade com as leis de prote\u00e7\u00e3o de dados, \ncomo o GDPR na Europa. \n4 Imparcialidade e n\u00e3o discrimina\u00e7\u00e3o: requisitos para evitar \nvieses e garantir que os LLMs n\u00e3o perpetuem a \ndiscrimina\u00e7\u00e3o e o vi\u00e9s, avaliando e corrigindo \nconstantemente seus algoritmos. \n4 Seguran\u00e7a e confiabilidade: requisitos de robustez \noperacional para evitar mau funcionamento ou manipula\u00e7\u00f5es \nque possam causar danos ou perda de informa\u00e7\u00f5es. \n4 Responsabilidade e governan\u00e7a: marco de responsabilidade \npara desenvolvedores e usu\u00e1rios de LLM em caso de danos \nou viola\u00e7\u00f5es de direitos, incluindo mecanismos de \nsupervis\u00e3o e controle. \n4 Supervis\u00e3o humana: a necessidade de manter uma \nsupervis\u00e3o humana eficaz sobre os LLMs, garantindo que \ndecis\u00f5es importantes possam ser revisadas e, se necess\u00e1rio, \ncorrigidas ou revertidas por humanos. \nEsses requisitos refletem um consenso emergente sobre os \nprinc\u00edpios fundamentais para o desenvolvimento seguro e \u00e9tico \ndo LLM e formam a base para futuras regulamenta\u00e7\u00f5es e \nadapta\u00e7\u00f5es espec\u00edficas \u00e0 medida que a tecnologia evolui.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2006, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"d0199156-0cef-449f-bd75-24640c930537": {"node_ids": ["cf272f44-9567-426d-9001-c1599af129d3"], "metadata": {"page_label": "1", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}}, "8b7fe797-6888-4f4a-af69-ee5bcee8de1b": {"node_ids": ["36f6ccf1-677e-4805-bdba-95c05a225d6b", "70fedd76-d69d-4a10-bb07-922154f5cdaf"], "metadata": {"page_label": "2", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}}, "d45043cd-1d20-4559-9bb4-c6bf9f6f0f33": {"node_ids": ["307696df-3841-49a1-ac19-9e2519f335ab", "c3745170-f446-48be-81f5-e977731f6a49"], "metadata": {"page_label": "3", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}}, "9c39d1b2-c70c-46be-93a0-0487312d6597": {"node_ids": ["e4232058-b208-477c-b22c-7b3e2a8a558d"], "metadata": {"page_label": "4", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}}, "2678d44b-7039-4edb-893b-42f1ba523e81": {"node_ids": ["8db86119-c30b-48fd-b457-c5dbd6203a5f"], "metadata": {"page_label": "5", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}}, "3f4c349a-d20b-497c-8ffe-8a99519cd0fc": {"node_ids": ["8cfc1a4e-d2e7-476a-8154-43496913601c", "6e5a57e1-8b38-4ffa-9c17-f38566cff29c"], "metadata": {"page_label": "6", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}}, "e6b8a447-7da3-46e4-a58e-3532ab371544": {"node_ids": ["f8fd9bb9-a80a-4369-ae94-74e2f8c2e655"], "metadata": {"page_label": "7", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}}, "c5369d4d-074a-48c1-b379-dc99fa7775fc": {"node_ids": ["cd99749c-6667-4f54-98c0-c6c91a8d1785"], "metadata": {"page_label": "8", "file_name": "LLM_2.pdf", "file_path": "../../data/LLM_2.pdf", "file_type": "application/pdf", "file_size": 427228, "creation_date": "2025-04-26", "last_modified_date": "2025-04-26"}}}}